{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b298860-8ae7-4686-807b-b6d7d0c9716b",
   "metadata": {},
   "source": [
    "# ML Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0c50c-af2c-47d6-9c31-7d3dd52d677b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958010d-0ed9-40e2-8a1f-85916ede2f5d",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44182d68-eb23-4b21-ad81-66957e153bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in ./env/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already up-to-date: numpy in ./env/lib/python3.8/site-packages (1.24.3)\n",
      "Requirement already up-to-date: matplotlib in ./env/lib/python3.8/site-packages (3.7.1)\n",
      "Requirement already up-to-date: tensorflow in ./env/lib/python3.8/site-packages (2.12.0)\n",
      "Requirement already up-to-date: tqdm in ./env/lib/python3.8/site-packages (4.65.0)\n",
      "Requirement already up-to-date: bs4 in ./env/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already up-to-date: IP2Location in ./env/lib/python3.8/site-packages (8.10.0)\n",
      "Requirement already up-to-date: chardet in ./env/lib/python3.8/site-packages (5.1.0)\n",
      "Requirement already up-to-date: scikit-learn in ./env/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in ./env/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in ./env/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in ./env/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in ./env/lib/python3.8/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-resources>=3.2.0; python_version < \"3.10\" in ./env/lib/python3.8/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied, skipping upgrade: fonttools>=4.22.0 in ./env/lib/python3.8/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in ./env/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.3.1 in ./env/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied, skipping upgrade: contourpy>=1.0.1 in ./env/lib/python3.8/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in ./env/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in ./env/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<1.15,>=1.11.0 in ./env/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in ./env/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in ./env/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied, skipping upgrade: keras<2.13,>=2.12.0 in ./env/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.6 in ./env/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py>=2.9.0 in ./env/lib/python3.8/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: jax>=0.3.15 in ./env/lib/python3.8/site-packages (from tensorflow) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.13,>=2.12.0 in ./env/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied, skipping upgrade: gast<=0.4.0,>=0.2.1 in ./env/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in ./env/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in ./env/lib/python3.8/site-packages (from tensorflow) (44.0.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers>=2.0 in ./env/lib/python3.8/site-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=1.0.0 in ./env/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0,>=1.24.3 in ./env/lib/python3.8/site-packages (from tensorflow) (1.54.0)\n",
      "Requirement already satisfied, skipping upgrade: astunparse>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: libclang>=13.0.0 in ./env/lib/python3.8/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<2.13,>=2.12 in ./env/lib/python3.8/site-packages (from tensorflow) (2.12.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./env/lib/python3.8/site-packages (from tensorflow) (4.22.3)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.1 in ./env/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\" in ./env/lib/python3.8/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in ./env/lib/python3.8/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.1.1 in ./env/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in ./env/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in ./env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=3.1.0; python_version < \"3.10\" in ./env/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.15.0)\n",
      "Requirement already satisfied, skipping upgrade: ml-dtypes>=0.0.3 in ./env/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel<1.0,>=0.23.0 in ./env/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<3,>=1.6.3 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=1.0.1 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<1.1,>=0.5 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.8.0,>=0.7.0 in ./env/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>1.2 in ./env/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in ./env/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in ./env/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in ./env/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in ./env/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=4.4; python_version < \"3.10\" in ./env/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.1.1 in ./env/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in ./env/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.6.0,>=0.4.6 in ./env/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in ./env/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!python3 -m pip install -U pandas numpy matplotlib tensorflow tqdm bs4 IP2Location chardet scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdab18-0c41-4e4b-866f-a540ccf1b8e2",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05022e96-8ef9-4e5f-80a6-3faa1d1d8810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 13:18:41.375099: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-25 13:18:41.456456: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-25 13:18:41.457934: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 13:18:42.662962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import socket\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import os\n",
    "import IP2Location\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"urllib3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8464ad-9975-4e2e-aafe-bbf0088a11be",
   "metadata": {},
   "source": [
    "## Download More Data / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586d269-d052-4278-83c5-aa50142987a7",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7622d0-b100-4b6d-b612-22911428a497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_CHUNK_SIZE = 0\n",
    "TIMEOUT = (10, 10)\n",
    "feature_names = [\"words\", \"aux\", \"city\", \"region\", \"country\", \"redirects\", \"latitude\", \"longitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4592025-f972-40b3-b3a6-bbab3d0c212c",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5154d2a2-1c48-4ec9-8e32-0e73bb1ebfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ip2location_database = IP2Location.IP2Location()\n",
    "ip2location_database.open(os.path.join(\"location_data\", \"IP2LOCATION-LITE-DB11.BIN\"))\n",
    "def get_location(ip_addr=None, hostname=None):\n",
    "    if(ip_addr is None and hostname is not None):\n",
    "        try:\n",
    "            ip_addr = socket.gethostbyname(hostname)\n",
    "        except socket.gaierror:\n",
    "            print(\"Skipped Location Download (Hostname Resolution Error for '\"+hostname+\"')\")\n",
    "            return None\n",
    "    location_data = ip2location_database.get_all(ip_addr)\n",
    "    if(location_data.country_short == \"-\"):\n",
    "        if(not ip_addr in location_database):\n",
    "            location_data = requests.get(\"https://ipinfo.io/\"+ip_addr+\"/json\").json()\n",
    "            if(\"error\" in location_data):\n",
    "                raise Exception(\"Failed because error with download (probably api quota exceded)\")\n",
    "            location_database[ip_addr] = location_data\n",
    "            location_database[ip_addr][\"country_short\"] = location_data[\"country\"]\n",
    "            location_database[ip_addr][\"latitude\"], location_database[ip_addr][\"longitude\"] = tuple(location_data[\"loc\"].split(\",\"))\n",
    "    else:\n",
    "        return ast.literal_eval(str(location_data))\n",
    "    return location_database[ip_addr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650c6240-a1d3-49ee-8166-7d3add259a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_url_from_relative(original_url, new_url):\n",
    "    if(new_url.startswith(\"javascript\")):\n",
    "        return None\n",
    "    if(new_url.startswith(\"http\")):\n",
    "        return new_url\n",
    "    url_with_scheme = \"http://\"+original_url if not original_url.startswith(\"http\") else original_url\n",
    "    parsed_url = urlparse(url_with_scheme)\n",
    "    url_scheme = parsed_url.scheme\n",
    "    url_host = parsed_url.netloc\n",
    "    \n",
    "    return urljoin(url_scheme+\"://\"+url_host, new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87109bb-9c26-4b78-a57f-28d53e6b4f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_aux_data(content, original_url):\n",
    "    aux = []\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href')\n",
    "        if(href is not None):\n",
    "            url = get_absolute_url_from_relative(original_url, href)\n",
    "            if(url is not None):\n",
    "                hostname = urlparse(url).netloc\n",
    "                json = get_location(hostname=hostname)\n",
    "                if(json is None):\n",
    "                    continue\n",
    "                try:\n",
    "                    columns = [\n",
    "                        \"hostname_\"+hostname,\n",
    "                        \"ip_addr_\"+json[\"ip\"],\n",
    "                        \"latitude_\"+str(int(float(json[\"latitude\"]))),\n",
    "                        \"longitude_\"+str(int(float(json[\"longitude\"]))),\n",
    "                        \"city_\"+json[\"city\"],\n",
    "                        \"region_\"+json[\"region\"],\n",
    "                        \"country_\"+json[\"country_short\"],\n",
    "                    ]\n",
    "                except KeyError:\n",
    "                    if(\"bogon\" in json and json[\"bogon\"]):\n",
    "                        print(\"Skipped Aux Data Download (Bogon IP)\")\n",
    "                    else:\n",
    "                        print(\"Problem with JSON: \", json)\n",
    "                    columns = []\n",
    "                \n",
    "                columns = [ re.compile('[\\W_]+').sub('_', column) for column in columns ]\n",
    "                \n",
    "                aux = aux + columns\n",
    "                \n",
    "    return \" \".join(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dfc97b4-6cad-49b4-bc67-bdb5916e5faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_one(original_url, timeout=TIMEOUT):\n",
    "    row = {}\n",
    "    row[\"attempted_download\"] = True\n",
    "\n",
    "    url_with_scheme = \"http://\"+original_url if not original_url.startswith(\"http\") else original_url\n",
    "    parsed_url = urlparse(url_with_scheme)\n",
    "    hostname = parsed_url.netloc\n",
    "    \n",
    "    try:\n",
    "        row[\"hostname\"] = hostname\n",
    "        \n",
    "        ip_addr = socket.gethostbyname(hostname)\n",
    "        row[\"ip_addr\"] = ip_addr\n",
    "\n",
    "        r = requests.get(url_with_scheme, verify=False, timeout=timeout)\n",
    "\n",
    "        row[\"status_code\"] = r.status_code\n",
    "        encoding = chardet.detect(r.content)['encoding']\n",
    "        #if(encoding == None):\n",
    "        #    print(\"Skipped content download (Decoding Error)\")\n",
    "        try:\n",
    "            content = r.content.decode(encoding)\n",
    "            row[\"content\"] = content\n",
    "        except:\n",
    "            try:\n",
    "                content = r.content.decode(\"utf\")\n",
    "            except:\n",
    "                content = None\n",
    "                print(\"Skipped Content Download (Decoding Error)\")\n",
    "            \n",
    "        if(content is not None):\n",
    "            row[\"aux\"] = download_aux_data(content, original_url)\n",
    "            row[\"words\"] = \" \".join(re.compile('[\\W_]+').sub(' ', BeautifulSoup(content, 'html.parser').get_text()).split(\" \"))\n",
    "        \n",
    "        redirects = 0\n",
    "        for r_history in r.history:\n",
    "            if(r_history.status_code == 301):\n",
    "                redirects = redirects + 1\n",
    "        row[\"redirects\"] = redirects\n",
    "\n",
    "        json = get_location(ip_addr=ip_addr)\n",
    "        if(json is None):\n",
    "            return row\n",
    "        try:\n",
    "            row[\"latitude\"] = json[\"latitude\"]\n",
    "            row[\"longitude\"] = json[\"longitude\"]\n",
    "            row[\"city\"] = json[\"city\"]\n",
    "            row[\"region\"] = json[\"region\"]\n",
    "            row[\"country\"] = json[\"country_short\"]\n",
    "\n",
    "        except KeyError:\n",
    "            if(\"bogon\" in json and json[\"bogon\"]):\n",
    "                print(\"Skipped Location Download (Bogon IP)\")\n",
    "            else:\n",
    "                print(\"Problem with JSON: \", json)\n",
    "        \n",
    "    except socket.gaierror:\n",
    "        print(\"Skipped (Hostname Resolution Error for '\"+hostname+\"')\")\n",
    "                      \n",
    "    except socket.error:\n",
    "        print(\"Skipped (Content Download Error for '\"+original_url+\"')\")\n",
    "        \n",
    "    except UnicodeError:\n",
    "        print(\"Skipped (Unicode Error for '\"+original_url+\"')\")\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddc8223-179e-4c0d-800e-ac4d542f1419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_chunk(chunk_size=DOWNLOAD_CHUNK_SIZE, timeout=TIMEOUT):\n",
    "    global raw_data\n",
    "    if(len(raw_data[raw_data[\"attempted_download\"] == True].index) != 0):\n",
    "          start = raw_data[raw_data[\"attempted_download\"] == True].index[-1]+1\n",
    "    end = start + chunk_size\n",
    "    end = end if len(raw_data[\"url\"]) > end else len(raw_data[\"url\"])\n",
    "    \n",
    "    print(\"Downloading %d more rows ([%d:%d])\" % (chunk_size, start, end))\n",
    "    for row_index in tqdm(range(start, end)):\n",
    "        row = download_one(raw_data.loc[row_index, \"url\"], timeout=timeout)\n",
    "        if(len(list(row)) > 0):\n",
    "            raw_data.loc[row_index, list(row)] = row.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f565d439-ff36-4cb4-9932-c34d8ceccddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global raw_data, location_database\n",
    "    \n",
    "    raw_data = pd.read_csv(\"./raw_data.csv\")\n",
    "    raw_data[\"attempted_download\"] = False\n",
    "    \n",
    "    try:\n",
    "        with open(\"./location_data.json\", 'r') as file:\n",
    "            location_database = json.load(file)\n",
    "        file.close()\n",
    "    except FileNotFoundError:\n",
    "        location_database = {}\n",
    "\n",
    "    try:\n",
    "        raw_data = pd.read_csv(\"./data.csv\", index_col=0, low_memory=False)\n",
    "        if(DOWNLOAD_CHUNK_SIZE > 0):\n",
    "            download_chunk()\n",
    "            raw_data.to_csv(\"./data.csv\")\n",
    "            with open(\"./location_data.json\", 'w') as file:\n",
    "                json.dump(location_database, file)\n",
    "            file.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        download_chunk()\n",
    "        raw_data.to_csv(\"./data.csv\")\n",
    "        with open(\"./location_data.json\", 'w') as file:\n",
    "            json.dump(location_database, file)\n",
    "        file.close()\n",
    "\n",
    "    data = raw_data.copy()\n",
    "\n",
    "    data = data.drop(\"attempted_download\", axis=1)\n",
    "    data = data[data[\"status_code\"] == 200]\n",
    "    data = data.drop(\"status_code\", axis=1)\n",
    "    data = data[feature_names + [\"type\", \"url\"]].dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data[\"redirects\"] = data[\"redirects\"].astype(str)\n",
    "    data[\"latitude\"] = data[\"latitude\"].astype(str)\n",
    "    data[\"longitude\"] = data[\"longitude\"].astype(str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72596d68-54bd-40d4-a875-58ef48bc4d5a",
   "metadata": {},
   "source": [
    "### Load/Download Data and display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05abf388-7d09-4fb1-8370-f8ee06be607a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>aux</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>redirects</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>larcadelcarnevale com Buy this domain larcade...</td>\n",
       "      <td>hostname_secure_voodoo_com ip_addr_192_64_146_...</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Bavaria</td>\n",
       "      <td>DE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.1374</td>\n",
       "      <td>11.5755</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://larcadelcarnevale.com/catalogo/palloncini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sorteo Notebook Diciembre 2012JavaScript isn t...</td>\n",
       "      <td>hostname_accounts_google_com ip_addr_142_251_1...</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>US</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.8951</td>\n",
       "      <td>-77.0364</td>\n",
       "      <td>phishing</td>\n",
       "      <td>https://docs.google.com/spreadsheet/viewform?f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shopper s Heaven 인터파크 홈 리빙 세탁 청소용품 리빙 최신 등록순 ...</td>\n",
       "      <td>hostname_interpark_com ip_addr_211_233_74_23 l...</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>KR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.566</td>\n",
       "      <td>126.9784</td>\n",
       "      <td>benign</td>\n",
       "      <td>http://interpark.com/displaycorner/FreeMarket....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Links KontaktAnfahrtDatenschutzImpressum Home...</td>\n",
       "      <td>hostname_www_pn_wuppertal_de ip_addr_217_160_0...</td>\n",
       "      <td>Karlsruhe</td>\n",
       "      <td>Baden-Wurttemberg</td>\n",
       "      <td>DE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0094</td>\n",
       "      <td>8.4044</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://www.pn-wuppertal.de/links/2-linkseite/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AfterMarket pl domena parafiapiaski pl Domena...</td>\n",
       "      <td>hostname_www_aftermarket_pl ip_addr_185_253_21...</td>\n",
       "      <td>Warsaw</td>\n",
       "      <td>Mazovia</td>\n",
       "      <td>PL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.2298</td>\n",
       "      <td>21.0118</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://www.parafiapiaski.pl/index.php?option=c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>WebopRo com is for sale HugeDomains Search 1 ...</td>\n",
       "      <td>hostname_www_HugeDomains_com ip_addr_104_26_6_...</td>\n",
       "      <td>Ashburn</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.039474</td>\n",
       "      <td>-77.491806</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://www.webopro.com/index.php/sits/pwws/119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Teaspoon A Man Who Wasn t There An Alt Series...</td>\n",
       "      <td>hostname_whofic_com ip_addr_179_61_137_3 latit...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.805269</td>\n",
       "      <td>-97.003601</td>\n",
       "      <td>benign</td>\n",
       "      <td>whofic.com/series.php?seriesid=1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Evri The New Hermes Cheap Parcel Delivery Cou...</td>\n",
       "      <td>hostname_evri_com ip_addr_45_60_6_42 latitude_...</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47.603909</td>\n",
       "      <td>-122.329842</td>\n",
       "      <td>benign</td>\n",
       "      <td>evri.com/organization/kukc-lp-0x1364cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>SelamSoft Amharic Dictionary A Your search wa...</td>\n",
       "      <td>hostname_amharicdictionary_com ip_addr_50_21_1...</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.96244</td>\n",
       "      <td>-75.199928</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://amharicdictionary.com/default.aspx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Accueil Loteries Loto Québec Aller au contenu...</td>\n",
       "      <td>hostname_diffusion_loto_quebec_com ip_addr_45_...</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.603909</td>\n",
       "      <td>-122.329842</td>\n",
       "      <td>benign</td>\n",
       "      <td>diffusion.loto-quebec.com/sw3/res/asp/index.as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words   \n",
       "0     larcadelcarnevale com Buy this domain larcade...  \\\n",
       "1    Sorteo Notebook Diciembre 2012JavaScript isn t...   \n",
       "2     Shopper s Heaven 인터파크 홈 리빙 세탁 청소용품 리빙 최신 등록순 ...   \n",
       "3     Links KontaktAnfahrtDatenschutzImpressum Home...   \n",
       "4     AfterMarket pl domena parafiapiaski pl Domena...   \n",
       "..                                                 ...   \n",
       "784   WebopRo com is for sale HugeDomains Search 1 ...   \n",
       "785   Teaspoon A Man Who Wasn t There An Alt Series...   \n",
       "786   Evri The New Hermes Cheap Parcel Delivery Cou...   \n",
       "787   SelamSoft Amharic Dictionary A Your search wa...   \n",
       "788   Accueil Loteries Loto Québec Aller au contenu...   \n",
       "\n",
       "                                                   aux          city   \n",
       "0    hostname_secure_voodoo_com ip_addr_192_64_146_...        Munich  \\\n",
       "1    hostname_accounts_google_com ip_addr_142_251_1...    Washington   \n",
       "2    hostname_interpark_com ip_addr_211_233_74_23 l...         Seoul   \n",
       "3    hostname_www_pn_wuppertal_de ip_addr_217_160_0...     Karlsruhe   \n",
       "4    hostname_www_aftermarket_pl ip_addr_185_253_21...        Warsaw   \n",
       "..                                                 ...           ...   \n",
       "784  hostname_www_HugeDomains_com ip_addr_104_26_6_...       Ashburn   \n",
       "785  hostname_whofic_com ip_addr_179_61_137_3 latit...      Victoria   \n",
       "786  hostname_evri_com ip_addr_45_60_6_42 latitude_...       Seattle   \n",
       "787  hostname_amharicdictionary_com ip_addr_50_21_1...  Philadelphia   \n",
       "788  hostname_diffusion_loto_quebec_com ip_addr_45_...       Seattle   \n",
       "\n",
       "                region country redirects   latitude    longitude        type   \n",
       "0              Bavaria      DE       0.0    48.1374      11.5755  defacement  \\\n",
       "1     Washington, D.C.      US       2.0    38.8951     -77.0364    phishing   \n",
       "2                Seoul      KR       0.0     37.566     126.9784      benign   \n",
       "3    Baden-Wurttemberg      DE       1.0    49.0094       8.4044  defacement   \n",
       "4              Mazovia      PL       0.0    52.2298      21.0118  defacement   \n",
       "..                 ...     ...       ...        ...          ...         ...   \n",
       "784           Virginia      US       0.0  39.039474   -77.491806  defacement   \n",
       "785              Texas      US       1.0  28.805269   -97.003601      benign   \n",
       "786         Washington      US       2.0  47.603909  -122.329842      benign   \n",
       "787       Pennsylvania      US       0.0   39.96244   -75.199928  defacement   \n",
       "788         Washington      US       1.0  47.603909  -122.329842      benign   \n",
       "\n",
       "                                                   url  \n",
       "0     http://larcadelcarnevale.com/catalogo/palloncini  \n",
       "1    https://docs.google.com/spreadsheet/viewform?f...  \n",
       "2    http://interpark.com/displaycorner/FreeMarket....  \n",
       "3    http://www.pn-wuppertal.de/links/2-linkseite/5...  \n",
       "4    http://www.parafiapiaski.pl/index.php?option=c...  \n",
       "..                                                 ...  \n",
       "784  http://www.webopro.com/index.php/sits/pwws/119...  \n",
       "785                whofic.com/series.php?seriesid=1745  \n",
       "786             evri.com/organization/kukc-lp-0x1364cd  \n",
       "787          http://amharicdictionary.com/default.aspx  \n",
       "788  diffusion.loto-quebec.com/sw3/res/asp/index.as...  \n",
       "\n",
       "[789 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af61902-6f5b-4fea-838f-b6fdb5d8bd09",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102e1ad-6327-4b86-bf5e-7c0dd3372cdb",
   "metadata": {},
   "source": [
    "### Define Hyperparameter Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44d20f1-7727-470a-9131-2a9da2cb21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 1000\n",
    "epochs = 25\n",
    "folds = 5\n",
    "batch_size = 1\n",
    "test_size = 0.33\n",
    "validation_size = 0.20 # ratio after test has been taken out\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b472d45-4a3a-4e4e-b4a6-a5ad9a7674bf",
   "metadata": {},
   "source": [
    "### Ensure Reproducibility (important for feature subsets comparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a197261d-e3d6-4ba7-9198-651bff55072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed():\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "#reset_random_seed()\n",
    "# Adapted from:\n",
    "# https://stackoverflow.com/questions/45230448/how-to-get-reproducible-result-when-running-keras-with-tensorflow-backend\n",
    "# AND\n",
    "# https://stackoverflow.com/questions/61078946/how-to-get-reproducible-results-keras-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea105ef9-3f9b-40f1-9c32-aa9dc62c754b",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83280a3b-80d7-45e6-aae9-2e2b3456dc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      #tf.keras.metrics.TruePositives(name='tp'),\n",
    "      #tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      #tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      #tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      #tf.keras.metrics.Precision(name='precision'),\n",
    "      #tf.keras.metrics.Recall(name='recall'),\n",
    "      #tf.keras.metrics.AUC(name='auc'),\n",
    "      #tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860ee41-387c-480f-a896-e90e934d8aa6",
   "metadata": {},
   "source": [
    "### Define string_lookup for One Hot Encoding the labels/type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b268c603-f12a-46ae-ab8f-b2b75046af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_lookup = tf.keras.layers.StringLookup(output_mode='one_hot')\n",
    "string_lookup.adapt(data[\"type\"]) #TODO: ensure this doesn't use efficelty use test dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c9f1f-9a6e-4b15-9d6f-606a8226addc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "713f79d6-63da-427c-9400-3b72721effaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return tf.strings.lower(input_data)\n",
    "\n",
    "def get_normalization_layer(feature_name):\n",
    "    normalization_layer = tf.keras.layers.Normalization(axis=None)\n",
    "    normalization_layer.adapt(data[feature_name].astype(np.float32)) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "    return normalization_layer\n",
    "\n",
    "def get_vectorize_layer(feature_name):\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length\n",
    "    )\n",
    "    vectorize_layer.adapt(data[feature_name]) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "    return vectorize_layer\n",
    "\n",
    "def get_string_lookup(feature_name):\n",
    "    lookup = tf.keras.layers.StringLookup(\n",
    "        output_mode='one_hot',\n",
    "        max_tokens=sequence_length,\n",
    "        pad_to_max_tokens=True,\n",
    "    )\n",
    "    lookup.adapt(data[feature_name]) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "236cb797-c4b7-470c-a085-367bc38dbb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5000), dtype=float32, numpy=\n",
       "array([[-0.01991978, -0.0091613 ,  0.02360929, ..., -0.034228  ,\n",
       "         0.0440397 ,  0.03828735],\n",
       "       [-0.01991978, -0.0091613 ,  0.02360929, ..., -0.034228  ,\n",
       "         0.0440397 ,  0.03828735],\n",
       "       [-0.01991978, -0.0091613 ,  0.02360929, ..., -0.034228  ,\n",
       "         0.0440397 ,  0.03828735]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Flatten()(tf.keras.layers.Embedding(max_features, 5)(get_vectorize_layer(\"aux\")([\"test\", \"tes\", \"hi\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056a205-7602-4def-a15d-3a51ff7f32c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Get Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0de43a0-0400-4385-8581-b05e11452db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(feature_names=feature_names):\n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(len(feature_names)):\n",
    "        \n",
    "        feature_name = feature_names[i]\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        if(feature_name == \"latitude\" or feature_name == \"longitude\"):\n",
    "            #outputs.append(flatten(get_normalization_layer(feature_name)(tf.strings.to_number(inputs[:,i], out_type=tf.dtypes.float32))))\n",
    "            inputs.append(tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name))\n",
    "            outputs.append(flatten(get_normalization_layer(feature_name)(tf.strings.to_number(inputs[i], out_type=tf.dtypes.float32))))\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            embedding_layer = tf.keras.layers.Embedding(max_features, 5)\n",
    "            if(feature_name == \"words\" or feature_name == \"aux\"):\n",
    "                layer = get_vectorize_layer(feature_name)\n",
    "            else:\n",
    "                layer = get_string_lookup(feature_name)\n",
    "            #outputs.append(tf.keras.layers.Flatten()(embedding_layer(layer(flatten(inputs[:,i])))))\n",
    "            \n",
    "            inputs.append(tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name))\n",
    "            outputs.append(flatten(embedding_layer(layer(tf.keras.layers.Flatten()(inputs[i])))))\n",
    "    \n",
    "    outputs = tf.concat(outputs, axis=-1)\n",
    "    \n",
    "\n",
    "    sequential_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.5),\n",
    "        #tf.keras.layers.Dense(5, activation='relu'),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=sequential_model(outputs) )\n",
    "\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    sequential_model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb32fe1-6370-4005-ad47-63ed0dbde6f9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0165526-74b7-4db9-80c0-e11f42352ddb",
   "metadata": {},
   "source": [
    "### Define Dataset Getter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c3e7e9f-c56e-4241-b0b0-4316076cc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data=data, feature_names=feature_names):\n",
    "    X = data[feature_names]\n",
    "    y = (string_lookup(list(data[\"type\"])))\n",
    "    #print(str(X.to_dict())[0:1000])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    train_and_validation, test = tf.keras.utils.split_dataset(dataset, right_size=test_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    train, validation = tf.keras.utils.split_dataset(dataset, right_size=validation_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    \n",
    "    print(train)\n",
    "    \n",
    "    return train, validation, test\n",
    "    #return dataset\n",
    "    \n",
    "    \n",
    "def get_X_data(data=data, feature_names=feature_names):\n",
    "    X = data[feature_names]\n",
    "    return X\n",
    "\n",
    "def get_y_data(data=data):\n",
    "    y = string_lookup(list(data[\"type\"]))\n",
    "    return y\n",
    "\n",
    "def get_split_datasets(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17400b5-ffa0-4889-be6f-6b5e195bf3e5",
   "metadata": {},
   "source": [
    "### Define Train Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45bcb0b8-12d0-4ff7-8f48-a81bfcbc0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(X, y):\n",
    "    X_out = {}\n",
    "    y_out = tf.reshape(y, (1,5,))\n",
    "    for i in range(len(feature_names)):\n",
    "        feature_name = feature_names[i]\n",
    "        #print(feature_name, tf.reshape(X[i], (1,)))\n",
    "        X_out[feature_name] = tf.reshape(X[i], (1,))\n",
    "    return X_out, y_out\n",
    "\n",
    "def train_model(model, train, validation):\n",
    "    #kfold = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    histories = []\n",
    "    \n",
    "    \n",
    "    y_classes = np.argmax(np.concatenate([y for _, y in train.map(gen)]), axis=1)\n",
    "    y_labels = np.unique(y_classes)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=y_labels, y=y_classes)\n",
    "    class_weights = dict(zip(y_labels, class_weights))\n",
    "    class_weights[0] = 0\n",
    "    \n",
    "    print(train.map(gen))\n",
    "    history = model.fit(\n",
    "        train.map(gen),\n",
    "        validation_data=validation.map(gen),\n",
    "        epochs=epochs,\n",
    "        #batch_size=batch_size,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)], #TODO: Try commenting this out\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    histories.append(history)\n",
    "        \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6251ebe-b280-47b9-a6eb-e119d8cdad3e",
   "metadata": {},
   "source": [
    "### Generate Feature Combination subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953808ee-0503-4d89-911d-bfd962447dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_combination_subsets():\n",
    "    feature_names_subsets = []\n",
    "    for N in range(len(feature_names) + 1):\n",
    "         for feature_names_subset in itertools.combinations(feature_names, N): # adapted from: https://stackoverflow.com/questions/464864/get-all-possible-2n-combinations-of-a-list-s-elements-of-any-length\n",
    "            feature_names_subset = list(feature_names_subset)\n",
    "            if(len(feature_names_subset) > 0):\n",
    "                feature_names_subsets.append(feature_names_subset)\n",
    "    return feature_names_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef16df9-f242-4927-80d9-585ae5e9b9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test all feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154036d6-a585-464e-a398-c8c69c48e757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b694bc23-7200-4884-856e-ee5c0ab4d8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                      | 0/254 [00:00<?, ?it/s]2023-04-25 13:42:22.079832: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [789,5]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(8,), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float32, name=None))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 13:42:22.666783: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [631,5]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "  0%|                                                                                                                                                                                      | 0/254 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'words': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b' Baseball Card Shopper Bob Dernier Baseball Card Prices Baseball card prices and checklists for Topps cards Bowman cards Fleer cards Donruss cards Leaf cards Baseball Cards By Year and Manufacturer Bob Dernier Available Bob Dernier Baseball Cards and Values View All Bob Dernier Cards Find the best deal on baseball cards you are searching for YearBrandCard PlayerTeamConditionPrice Value 1983 Topps 43 Bob Dernier Phillies nrmt mt 0 50 1984 Topps 358 Bob Dernier Phillies nrmt mt 0 50 1985 Topps 589 Bob Dernier Cubs nrmt mt 0 50 1986 Topps 188 Bob Dernier Cubs nrmt mt 0 50 1987 Topps 715 Bob Dernier Cubs nrmt mt 0 40 0 50 1989 Topps 418 Bob Dernier Phillies nrmt mt 0 50 1990 Topps 204 Bob Dernier Phillies nrmt mt 0 40 0 50 Home Terms of Use Privacy Policy Site Map Contact Us BaseballCardShopper com does not display complete baseball card prices and card listings for any vendor We update price listings regularly but vendor prices are subject to change and BaseballCardShopper com may not have a vendor s most current prices This web site is sponsored by PearsonSportsCards com BaseballCardShopper com for baseball card shoppers Copyright 2008 2010 Shad Island LTD This baseball card price site covers Topps Bowman Leaf Fleer Donruss Upper Deck Custom Search To search for a card Click Here Home Baseball Cards By Year and Manufacturer By Team Players A C Players D G Players H M Players N S Players T Z Card Search Collecting Card Collecting Resources Rookie Card Favorites Best Packs Blog Members NEW Log On Members Dugout '],\n",
      "      dtype=object)>, 'aux': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_blog_baseballcardshopper_com ip_addr_208_109_98_37 latitude_33 longitude_111 city_Tempe region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US hostname_baseballcardshopper_com ip_addr_50_62_160_77 latitude_33 longitude_112 city_Phoenix region_Arizona country_US'],\n",
      "      dtype=object)>, 'city': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Phoenix'], dtype=object)>, 'region': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Arizona'], dtype=object)>, 'country': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'US'], dtype=object)>, 'redirects': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'0.0'], dtype=object)>, 'latitude': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'33.44838'], dtype=object)>, 'longitude': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'-112.074043'], dtype=object)>}, {'words': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'Home Recent Theatre THE SWEET SCIENCE OF BRUISING at Wilton s Music HallHomeActingTV Film TV Media Archive TheatreDirecting TVPhotosVoiceWikiMoreHomeActingTV Film TV Media Archive TheatreDirecting TVPhotosVoiceWikiHomeActingTV Film TV Media Archive TheatreDirecting TVPhotosVoiceWikiOwen BrenmanOwen BrenmanOwen BrenmanOwen BrenmanActor director Official website Actor director Official website Actor director Official website Diamond Management WelcomeRecently played professor Charlie Sharp in The Sweet Science of Bruising at Wilton s Music Hall Before that Dr Heston Carter in the BBC drama series Doctors 10 yrs which I also directed You may also know me from playing Nick Swainey in the multi award winning BBC sitcom One Foot in the Grave as well as many other credits Currently filming Sister Boniface Mysteries for the BBC 2022 Wiki showreel media interviews Social Contact MeSend MessageNameEmail SendThis site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply HomeOwen BrenmanCopyright 2022 Owen Brenman All Rights Reserved This website uses cookies We use cookies to analyze website traffic and optimize your website experience By accepting our use of cookies your data will be aggregated with all other user data DeclineAccept '],\n",
      "      dtype=object)>, 'aux': <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'hostname_wiltons_org_uk ip_addr_178_162_201_225 latitude_50 longitude_8 city_Frankfurt_am_Main region_Hessen country_DE hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_en_m_wikipedia_org ip_addr_208_80_154_224 latitude_37 longitude_122 city_San_Francisco region_California country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_en_m_wikipedia_org ip_addr_208_80_154_224 latitude_37 longitude_122 city_San_Francisco region_California country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle region_Washington country_US hostname_owenbrenman_com ip_addr_13_248_243_5 latitude_47 longitude_122 city_Seattle reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_models\n",
    "models = []\n",
    "feature_names_subsets = generate_feature_combination_subsets()\n",
    "for i in tqdm(range(len(feature_names_subsets)-1, 0, -1)):\n",
    "    #reset_random_seed()\n",
    "    feature_names_subset = feature_names_subsets[i]\n",
    "    #print(feature_names_subset)\n",
    "    #print(\"New Model: \"+\",\".join(feature_names_subset))\n",
    "    \n",
    "    train, validation, test = get_dataset(data, feature_names_subset)\n",
    "    \n",
    "    print( str([ X for X, y in train.map(gen) ])[0:10000] )\n",
    "    #model = get_model(feature_names_subset)\n",
    "    #histories = train_model(model, train, validation)\n",
    "    #model.evaluate(test.map(gen))\n",
    "    #model.save(\"saved_model/\"+\",\".join(feature_names_subset))\n",
    "    break\n",
    "    \n",
    "    #X = get_X_data(data, feature_names_subset)\n",
    "    # y = get_y_data(data)\n",
    "    # X_train, X_test, y_train, y_test = get_split_datasets(X, y)\n",
    "    # model = get_model(feature_names_subset)\n",
    "    # histories = train_model(model, X_train, y_train)\n",
    "    # model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    # tf.saved_model.save(model, \"saved_model/\"+\",\".join(feature_names_subset))\n",
    "    # models.append({\"features_names_subset\":feature_names_subset, \"model\":model, \"histories\":histories})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801e3c8-b5f8-4ed8-a36b-3c7130a18470",
   "metadata": {},
   "source": [
    "## Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c29e03-cf61-4e03-bec1-ef41dd0577f5",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249da21a-b267-4790-874b-c43efa654af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding_to_type_string(one_hot_encoding):\n",
    "    return string_lookup.get_vocabulary()[np.argmax(tf.nn.softmax(one_hot_encoding))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd6dc4-be10-43db-abd8-7bbbb2578cf6",
   "metadata": {},
   "source": [
    "### Define visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7df47c-8b0d-4b44-bfc3-f8d08f731843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_epochs_vs_accuracy_and_loss(model, histories):\n",
    "    acc = []\n",
    "    val_acc = []\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for history in histories:\n",
    "        acc = acc + history.history['accuracy']\n",
    "        val_acc = val_acc + history.history['val_accuracy']\n",
    "\n",
    "        loss = loss + history.history['loss']\n",
    "        val_loss = val_loss + history.history['val_loss']\n",
    "\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    pyplot.figure(figsize=(8, 8))\n",
    "    pyplot.subplot(1, 2, 1)\n",
    "    pyplot.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    pyplot.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    pyplot.legend(loc='lower right')\n",
    "    pyplot.title('Training and Validation Accuracy')\n",
    "\n",
    "    pyplot.subplot(1, 2, 2)\n",
    "    pyplot.plot(epochs_range, loss, label='Training Loss')\n",
    "    pyplot.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.title('Training and Validation Loss')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf16f3e-a940-453e-b25e-f41bee3558e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_prediction_vs_actual_dataframe_table(model):\n",
    "    urls = [ data.loc[i, \"url\"] for i in range(len(X_test)) ]\n",
    "    predictions = [ one_hot_encoding_to_type_string(model.predict(X_test)[i]) for i in range(len(X_test)) ]\n",
    "    actuals = [ one_hot_encoding_to_type_string(y_test[i]) for i in range(len(y_test)) ]\n",
    "\n",
    "    df = pd.DataFrame(np.array([urls, actuals, predictions]).T, columns=[\"url\", \"actual\", \"predicted\"])\n",
    "    df.reset_index(drop=True)\n",
    "    return df[df[\"actual\"] == df[\"predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b26e8b-8e1f-4340-a3bb-fb4c17586be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    features_names_subset = m[\"features_names_subset\"]\n",
    "    model = m[\"model\"]\n",
    "    histories = m[\"histories\"]\n",
    "    print(features_names_subset)\n",
    "    plot_epochs_vs_accuracy_and_loss(model, histories)\n",
    "    show_prediction_vs_actual_dataframe_table(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c272e3f-567b-4f2d-8fc6-f7ad5c7bbc9b",
   "metadata": {},
   "source": [
    "## Predict More (Test) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a0ffd-49fc-4b77-92ad-9a30c7166d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(url):\n",
    "    one = download_one(url)\n",
    "    if(one is None or one[\"status_code\"] != 200):\n",
    "        return \"Failed to get data for '\" + url + \"'\"\n",
    "    one = pd.DataFrame([one.values()], columns=list(one))\n",
    "    one[\"redirects\"] = one[\"redirects\"].astype(str)\n",
    "    one[\"latitude\"] = one[\"latitude\"].astype(str)\n",
    "    one[\"longitude\"] = one[\"longitude\"].astype(str)\n",
    "    \n",
    "    outputs = []\n",
    "    for m in models:\n",
    "        features_names_subset = m[\"features_names_subset\"]\n",
    "        model = m[\"model\"]\n",
    "        histories = m[\"histories\"]\n",
    "        outputs.append(features_names_subset+\": \"+one_hot_encoding_to_type_string(model.predict(one[feature_names])))\n",
    "    return \"\\n\".join(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddcbe0-5843-466a-9dc3-97cf01c9121c",
   "metadata": {},
   "source": [
    "### Test URLs not in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6e37d-fffd-4358-934d-a794d8d6661e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict(\"https://cnn.com/\"))\n",
    "print(predict(\"https://google.com/\"))\n",
    "print(predict(\"https://disneyplus.com/\"))\n",
    "print(predict(\"https://uvm.edu/\"))\n",
    "print(predict(\"https://en.wikipedia.org/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb8d5-801a-4d0b-b298-a047b04217d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict(\"http://www.fwuits.monster/5576HU239G5U8qn613JT25e29A512cY17UFEGsvtwfDm-mEGsi8dRhomSdo6q_s1v06SJOHzt/caretaker-instrumentally\"))\n",
    "print(predict(\"http://www.luieis.beauty/attendee-sinfulness/d8c5V2z395ig86k13i21f62LK4ee2s17QFEGsvtwfDm-mEGsi11dR0om9Reoje6Vok1B08jUHtp1oL\"))\n",
    "print(predict(\"http://www.eiyeid.monster/e3f6K2Q3v95QF8u617c_OTR21f2Peo4ed9T17tFEGsvtwfDm-mEGsi11zRTom9QKoMe6V1Q0_m7aXXqHto/telephoner-plantation\"))\n",
    "print(predict(\"http://irever.live/r.php?q=NzkyNjM0NDs2MTg3OTsxMDA2OzM0OzI7MjAyMy0wMy0yMSAxNTozMDowNTs5OzE7bDs7\"))\n",
    "print(predict(\"https://click.yescaloriedietplan.com/?t=c&ids=NDM5NjExOTY3__NDIxMg%3D%3D__OTg0ODE1ODc%3D__OTAx__1102&url=aHR0cHMlM0ElMkYlMkZ0cmFja2luZy5oZWFsdGhpZXJsaWZ0LmNvbSUyRnpwdGk=\"))\n",
    "print(predict(\"https://links.besttacticalknife.net/a/1625/click/4198/483611/747064e7195ac13a0f72ab11a22cb3f2ad8fa72c/4ec46075f0b8612b13e660aefd0511d442a49206\"))\n",
    "print(predict(\"http://www.folifort.email/l/lt1CK11667E122SG/5494NX7579O10146YB349N80481934SK3221472804\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5662e41-61c1-4340-bc6d-c608377fcfcb",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Train and test a model on every combination of features to see what combinations are most important. - In Progress\n",
    "2. Create a constant seed to reduce random noise-based accuracy changing between models - Done\n",
    "3. Crate a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efc32b-bff3-4c9a-9957-07b308a31746",
   "metadata": {},
   "source": [
    "## Notes for Writeup\n",
    "### Limitation\n",
    "1. Redirects being treated as catagorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18661d-41c8-4305-af53-5daad009dede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
