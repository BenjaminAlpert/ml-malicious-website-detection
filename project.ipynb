{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b298860-8ae7-4686-807b-b6d7d0c9716b",
   "metadata": {},
   "source": [
    "# ML Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0c50c-af2c-47d6-9c31-7d3dd52d677b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958010d-0ed9-40e2-8a1f-85916ede2f5d",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44182d68-eb23-4b21-ad81-66957e153bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python3 -m pip install pandas numpy matplotlib tensorflow-cpu tqdm bs4 IP2Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdab18-0c41-4e4b-866f-a540ccf1b8e2",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05022e96-8ef9-4e5f-80a6-3faa1d1d8810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import socket\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import os\n",
    "import IP2Location\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"urllib3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8464ad-9975-4e2e-aafe-bbf0088a11be",
   "metadata": {},
   "source": [
    "## Download More Data / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586d269-d052-4278-83c5-aa50142987a7",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7622d0-b100-4b6d-b612-22911428a497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_CHUNK_SIZE = 0\n",
    "TIMEOUT = (10, 10)\n",
    "feature_names = [\"words\", \"aux\", \"city\", \"region\", \"country\", \"redirects\", \"latitude\", \"longitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4592025-f972-40b3-b3a6-bbab3d0c212c",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5154d2a2-1c48-4ec9-8e32-0e73bb1ebfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ip2location_database = IP2Location.IP2Location()\n",
    "ip2location_database.open(os.path.join(\"location_data\", \"IP2LOCATION-LITE-DB11.BIN\"))\n",
    "def get_location(ip_addr=None, hostname=None):\n",
    "    if(ip_addr is None and hostname is not None):\n",
    "        try:\n",
    "            ip_addr = socket.gethostbyname(hostname)\n",
    "        except socket.gaierror:\n",
    "            print(\"Skipped Location Download (Hostname Resolution Error for '\"+hostname+\"')\")\n",
    "            return None\n",
    "    location_data = ip2location_database.get_all(ip_addr)\n",
    "    if(location_data.country_short == \"-\"):\n",
    "        if(not ip_addr in location_database):\n",
    "            location_data = requests.get(\"https://ipinfo.io/\"+ip_addr+\"/json\").json()\n",
    "            if(\"error\" in location_data):\n",
    "                raise Exception(\"Failed because error with download (probably api quota exceded)\")\n",
    "            location_database[ip_addr] = location_data\n",
    "            location_database[ip_addr][\"country_short\"] = location_data[\"country\"]\n",
    "            location_database[ip_addr][\"latitude\"], location_database[ip_addr][\"longitude\"] = tuple(location_data[\"loc\"].split(\",\"))\n",
    "    else:\n",
    "        return ast.literal_eval(str(location_data))\n",
    "    return location_database[ip_addr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650c6240-a1d3-49ee-8166-7d3add259a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_url_from_relative(original_url, new_url):\n",
    "    if(new_url.startswith(\"javascript\")):\n",
    "        return None\n",
    "    if(new_url.startswith(\"http\")):\n",
    "        return new_url\n",
    "    url_with_scheme = \"http://\"+original_url if not original_url.startswith(\"http\") else original_url\n",
    "    parsed_url = urlparse(url_with_scheme)\n",
    "    url_scheme = parsed_url.scheme\n",
    "    url_host = parsed_url.netloc\n",
    "    \n",
    "    return urljoin(url_scheme+\"://\"+url_host, new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87109bb-9c26-4b78-a57f-28d53e6b4f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_aux_data(content, original_url):\n",
    "    aux = []\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href')\n",
    "        if(href is not None):\n",
    "            url = get_absolute_url_from_relative(original_url, href)\n",
    "            if(url is not None):\n",
    "                hostname = urlparse(url).netloc\n",
    "                json = get_location(hostname=hostname)\n",
    "                if(json is None):\n",
    "                    continue\n",
    "                try:\n",
    "                    columns = [\n",
    "                        \"hostname_\"+hostname,\n",
    "                        \"ip_addr_\"+json[\"ip\"],\n",
    "                        \"latitude_\"+str(int(float(json[\"latitude\"]))),\n",
    "                        \"longitude_\"+str(int(float(json[\"longitude\"]))),\n",
    "                        \"city_\"+json[\"city\"],\n",
    "                        \"region_\"+json[\"region\"],\n",
    "                        \"country_\"+json[\"country_short\"],\n",
    "                    ]\n",
    "                except KeyError:\n",
    "                    if(\"bogon\" in json and json[\"bogon\"]):\n",
    "                        print(\"Skipped Aux Data Download (Bogon IP)\")\n",
    "                    else:\n",
    "                        print(\"Problem with JSON: \", json)\n",
    "                    columns = []\n",
    "                \n",
    "                columns = [ re.compile('[\\W_]+').sub('_', column) for column in columns ]\n",
    "                \n",
    "                aux = aux + columns\n",
    "                \n",
    "    return \" \".join(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dfc97b4-6cad-49b4-bc67-bdb5916e5faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_one(original_url, timeout=TIMEOUT):\n",
    "    row = {}\n",
    "    row[\"attempted_download\"] = True\n",
    "\n",
    "    url_with_scheme = \"http://\"+original_url if not original_url.startswith(\"http\") else original_url\n",
    "    parsed_url = urlparse(url_with_scheme)\n",
    "    hostname = parsed_url.netloc\n",
    "    \n",
    "    try:\n",
    "        row[\"hostname\"] = hostname\n",
    "        \n",
    "        ip_addr = socket.gethostbyname(hostname)\n",
    "        row[\"ip_addr\"] = ip_addr\n",
    "\n",
    "        r = requests.get(url_with_scheme, verify=False, timeout=timeout)\n",
    "\n",
    "        row[\"status_code\"] = r.status_code\n",
    "        encoding = chardet.detect(r.content)['encoding']\n",
    "        #if(encoding == None):\n",
    "        #    print(\"Skipped content download (Decoding Error)\")\n",
    "        try:\n",
    "            content = r.content.decode(encoding)\n",
    "            row[\"content\"] = content\n",
    "        except:\n",
    "            try:\n",
    "                content = r.content.decode(\"utf\")\n",
    "            except:\n",
    "                content = None\n",
    "                print(\"Skipped Content Download (Decoding Error)\")\n",
    "            \n",
    "        if(content is not None):\n",
    "            row[\"aux\"] = download_aux_data(content, original_url)\n",
    "            row[\"words\"] = \" \".join(re.compile('[\\W_]+').sub(' ', BeautifulSoup(content, 'html.parser').get_text()).split(\" \"))\n",
    "        \n",
    "        redirects = 0\n",
    "        for r_history in r.history:\n",
    "            if(r_history.status_code == 301):\n",
    "                redirects = redirects + 1\n",
    "        row[\"redirects\"] = redirects\n",
    "\n",
    "        json = get_location(ip_addr=ip_addr)\n",
    "        if(json is None):\n",
    "            return row\n",
    "        try:\n",
    "            row[\"latitude\"] = json[\"latitude\"]\n",
    "            row[\"longitude\"] = json[\"longitude\"]\n",
    "            row[\"city\"] = json[\"city\"]\n",
    "            row[\"region\"] = json[\"region\"]\n",
    "            row[\"country\"] = json[\"country_short\"]\n",
    "\n",
    "        except KeyError:\n",
    "            if(\"bogon\" in json and json[\"bogon\"]):\n",
    "                print(\"Skipped Location Download (Bogon IP)\")\n",
    "            else:\n",
    "                print(\"Problem with JSON: \", json)\n",
    "        \n",
    "    except socket.gaierror:\n",
    "        print(\"Skipped (Hostname Resolution Error for '\"+hostname+\"')\")\n",
    "                      \n",
    "    except socket.error:\n",
    "        print(\"Skipped (Content Download Error for '\"+original_url+\"')\")\n",
    "        \n",
    "    except UnicodeError:\n",
    "        print(\"Skipped (Unicode Error for '\"+original_url+\"')\")\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddc8223-179e-4c0d-800e-ac4d542f1419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_chunk(chunk_size=DOWNLOAD_CHUNK_SIZE, timeout=TIMEOUT):\n",
    "    global raw_data\n",
    "    if(len(raw_data[raw_data[\"attempted_download\"] == True].index) != 0):\n",
    "          start = raw_data[raw_data[\"attempted_download\"] == True].index[-1]+1\n",
    "    end = start + chunk_size\n",
    "    end = end if len(raw_data[\"url\"]) > end else len(raw_data[\"url\"])\n",
    "    \n",
    "    print(\"Downloading %d more rows ([%d:%d])\" % (chunk_size, start, end))\n",
    "    for row_index in tqdm(range(start, end)):\n",
    "        row = download_one(raw_data.loc[row_index, \"url\"], timeout=timeout)\n",
    "        if(len(list(row)) > 0):\n",
    "            raw_data.loc[row_index, list(row)] = row.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f565d439-ff36-4cb4-9932-c34d8ceccddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global raw_data, location_database\n",
    "    \n",
    "    raw_data = pd.read_csv(\"./raw_data.csv\")\n",
    "    raw_data[\"attempted_download\"] = False\n",
    "    \n",
    "    try:\n",
    "        with open(\"./location_data.json\", 'r') as file:\n",
    "            location_database = json.load(file)\n",
    "        file.close()\n",
    "    except FileNotFoundError:\n",
    "        location_database = {}\n",
    "\n",
    "    try:\n",
    "        raw_data = pd.read_csv(\"./data.csv\", index_col=0, low_memory=False)\n",
    "        if(DOWNLOAD_CHUNK_SIZE > 0):\n",
    "            download_chunk()\n",
    "            raw_data.to_csv(\"./data.csv\")\n",
    "            with open(\"./location_data.json\", 'w') as file:\n",
    "                json.dump(location_database, file)\n",
    "            file.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        download_chunk()\n",
    "        raw_data.to_csv(\"./data.csv\")\n",
    "        with open(\"./location_data.json\", 'w') as file:\n",
    "            json.dump(location_database, file)\n",
    "        file.close()\n",
    "\n",
    "    data = raw_data.copy()\n",
    "\n",
    "    data = data.drop(\"attempted_download\", axis=1)\n",
    "    data = data[data[\"status_code\"] == 200]\n",
    "    data = data.drop(\"status_code\", axis=1)\n",
    "    data = data[feature_names + [\"type\", \"url\"]].dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data[\"redirects\"] = data[\"redirects\"].astype(str)\n",
    "    data[\"latitude\"] = data[\"latitude\"].astype(str)\n",
    "    data[\"longitude\"] = data[\"longitude\"].astype(str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72596d68-54bd-40d4-a875-58ef48bc4d5a",
   "metadata": {},
   "source": [
    "### Load/Download Data and display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05abf388-7d09-4fb1-8370-f8ee06be607a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>aux</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>redirects</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>larcadelcarnevale com Buy this domain larcade...</td>\n",
       "      <td>hostname_secure_voodoo_com ip_addr_192_64_146_...</td>\n",
       "      <td>Munich</td>\n",
       "      <td>Bavaria</td>\n",
       "      <td>DE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.1374</td>\n",
       "      <td>11.5755</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://larcadelcarnevale.com/catalogo/palloncini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sorteo Notebook Diciembre 2012JavaScript isn t...</td>\n",
       "      <td>hostname_accounts_google_com ip_addr_142_251_1...</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>US</td>\n",
       "      <td>2.0</td>\n",
       "      <td>38.8951</td>\n",
       "      <td>-77.0364</td>\n",
       "      <td>phishing</td>\n",
       "      <td>https://docs.google.com/spreadsheet/viewform?f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shopper s Heaven 인터파크 홈 리빙 세탁 청소용품 리빙 최신 등록순 ...</td>\n",
       "      <td>hostname_interpark_com ip_addr_211_233_74_23 l...</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>Seoul</td>\n",
       "      <td>KR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.566</td>\n",
       "      <td>126.9784</td>\n",
       "      <td>benign</td>\n",
       "      <td>http://interpark.com/displaycorner/FreeMarket....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Links KontaktAnfahrtDatenschutzImpressum Home...</td>\n",
       "      <td>hostname_www_pn_wuppertal_de ip_addr_217_160_0...</td>\n",
       "      <td>Karlsruhe</td>\n",
       "      <td>Baden-Wurttemberg</td>\n",
       "      <td>DE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0094</td>\n",
       "      <td>8.4044</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://www.pn-wuppertal.de/links/2-linkseite/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AfterMarket pl domena parafiapiaski pl Domena...</td>\n",
       "      <td>hostname_www_aftermarket_pl ip_addr_185_253_21...</td>\n",
       "      <td>Warsaw</td>\n",
       "      <td>Mazovia</td>\n",
       "      <td>PL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.2298</td>\n",
       "      <td>21.0118</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://www.parafiapiaski.pl/index.php?option=c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>WebopRo com is for sale HugeDomains Search 1 ...</td>\n",
       "      <td>hostname_www_HugeDomains_com ip_addr_104_26_6_...</td>\n",
       "      <td>Ashburn</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.039474</td>\n",
       "      <td>-77.491806</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://www.webopro.com/index.php/sits/pwws/119...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Teaspoon A Man Who Wasn t There An Alt Series...</td>\n",
       "      <td>hostname_whofic_com ip_addr_179_61_137_3 latit...</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.805269</td>\n",
       "      <td>-97.003601</td>\n",
       "      <td>benign</td>\n",
       "      <td>whofic.com/series.php?seriesid=1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Evri The New Hermes Cheap Parcel Delivery Cou...</td>\n",
       "      <td>hostname_evri_com ip_addr_45_60_6_42 latitude_...</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47.603909</td>\n",
       "      <td>-122.329842</td>\n",
       "      <td>benign</td>\n",
       "      <td>evri.com/organization/kukc-lp-0x1364cd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>SelamSoft Amharic Dictionary A Your search wa...</td>\n",
       "      <td>hostname_amharicdictionary_com ip_addr_50_21_1...</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.96244</td>\n",
       "      <td>-75.199928</td>\n",
       "      <td>defacement</td>\n",
       "      <td>http://amharicdictionary.com/default.aspx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Accueil Loteries Loto Québec Aller au contenu...</td>\n",
       "      <td>hostname_diffusion_loto_quebec_com ip_addr_45_...</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.603909</td>\n",
       "      <td>-122.329842</td>\n",
       "      <td>benign</td>\n",
       "      <td>diffusion.loto-quebec.com/sw3/res/asp/index.as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words  \\\n",
       "0     larcadelcarnevale com Buy this domain larcade...   \n",
       "1    Sorteo Notebook Diciembre 2012JavaScript isn t...   \n",
       "2     Shopper s Heaven 인터파크 홈 리빙 세탁 청소용품 리빙 최신 등록순 ...   \n",
       "3     Links KontaktAnfahrtDatenschutzImpressum Home...   \n",
       "4     AfterMarket pl domena parafiapiaski pl Domena...   \n",
       "..                                                 ...   \n",
       "784   WebopRo com is for sale HugeDomains Search 1 ...   \n",
       "785   Teaspoon A Man Who Wasn t There An Alt Series...   \n",
       "786   Evri The New Hermes Cheap Parcel Delivery Cou...   \n",
       "787   SelamSoft Amharic Dictionary A Your search wa...   \n",
       "788   Accueil Loteries Loto Québec Aller au contenu...   \n",
       "\n",
       "                                                   aux          city  \\\n",
       "0    hostname_secure_voodoo_com ip_addr_192_64_146_...        Munich   \n",
       "1    hostname_accounts_google_com ip_addr_142_251_1...    Washington   \n",
       "2    hostname_interpark_com ip_addr_211_233_74_23 l...         Seoul   \n",
       "3    hostname_www_pn_wuppertal_de ip_addr_217_160_0...     Karlsruhe   \n",
       "4    hostname_www_aftermarket_pl ip_addr_185_253_21...        Warsaw   \n",
       "..                                                 ...           ...   \n",
       "784  hostname_www_HugeDomains_com ip_addr_104_26_6_...       Ashburn   \n",
       "785  hostname_whofic_com ip_addr_179_61_137_3 latit...      Victoria   \n",
       "786  hostname_evri_com ip_addr_45_60_6_42 latitude_...       Seattle   \n",
       "787  hostname_amharicdictionary_com ip_addr_50_21_1...  Philadelphia   \n",
       "788  hostname_diffusion_loto_quebec_com ip_addr_45_...       Seattle   \n",
       "\n",
       "                region country redirects   latitude    longitude        type  \\\n",
       "0              Bavaria      DE       0.0    48.1374      11.5755  defacement   \n",
       "1     Washington, D.C.      US       2.0    38.8951     -77.0364    phishing   \n",
       "2                Seoul      KR       0.0     37.566     126.9784      benign   \n",
       "3    Baden-Wurttemberg      DE       1.0    49.0094       8.4044  defacement   \n",
       "4              Mazovia      PL       0.0    52.2298      21.0118  defacement   \n",
       "..                 ...     ...       ...        ...          ...         ...   \n",
       "784           Virginia      US       0.0  39.039474   -77.491806  defacement   \n",
       "785              Texas      US       1.0  28.805269   -97.003601      benign   \n",
       "786         Washington      US       2.0  47.603909  -122.329842      benign   \n",
       "787       Pennsylvania      US       0.0   39.96244   -75.199928  defacement   \n",
       "788         Washington      US       1.0  47.603909  -122.329842      benign   \n",
       "\n",
       "                                                   url  \n",
       "0     http://larcadelcarnevale.com/catalogo/palloncini  \n",
       "1    https://docs.google.com/spreadsheet/viewform?f...  \n",
       "2    http://interpark.com/displaycorner/FreeMarket....  \n",
       "3    http://www.pn-wuppertal.de/links/2-linkseite/5...  \n",
       "4    http://www.parafiapiaski.pl/index.php?option=c...  \n",
       "..                                                 ...  \n",
       "784  http://www.webopro.com/index.php/sits/pwws/119...  \n",
       "785                whofic.com/series.php?seriesid=1745  \n",
       "786             evri.com/organization/kukc-lp-0x1364cd  \n",
       "787          http://amharicdictionary.com/default.aspx  \n",
       "788  diffusion.loto-quebec.com/sw3/res/asp/index.as...  \n",
       "\n",
       "[789 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af61902-6f5b-4fea-838f-b6fdb5d8bd09",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102e1ad-6327-4b86-bf5e-7c0dd3372cdb",
   "metadata": {},
   "source": [
    "### Define Hyperparameter Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44d20f1-7727-470a-9131-2a9da2cb21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 1000\n",
    "epochs = 25\n",
    "folds = 5\n",
    "batch_size = 1\n",
    "test_size = 0.33\n",
    "validation_size = 0.20 # ratio after test has been taken out\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b472d45-4a3a-4e4e-b4a6-a5ad9a7674bf",
   "metadata": {},
   "source": [
    "### Ensure Reproducibility (important for feature subsets comparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a197261d-e3d6-4ba7-9198-651bff55072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed():\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "#reset_random_seed()\n",
    "# Adapted from:\n",
    "# https://stackoverflow.com/questions/45230448/how-to-get-reproducible-result-when-running-keras-with-tensorflow-backend\n",
    "# AND\n",
    "# https://stackoverflow.com/questions/61078946/how-to-get-reproducible-results-keras-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea105ef9-3f9b-40f1-9c32-aa9dc62c754b",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83280a3b-80d7-45e6-aae9-2e2b3456dc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      #tf.keras.metrics.TruePositives(name='tp'),\n",
    "      #tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      #tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      #tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      #tf.keras.metrics.Precision(name='precision'),\n",
    "      #tf.keras.metrics.Recall(name='recall'),\n",
    "      #tf.keras.metrics.AUC(name='auc'),\n",
    "      #tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860ee41-387c-480f-a896-e90e934d8aa6",
   "metadata": {},
   "source": [
    "### Define string_lookup for One Hot Encoding the labels/type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b268c603-f12a-46ae-ab8f-b2b75046af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_lookup = tf.keras.layers.StringLookup(output_mode='one_hot')\n",
    "string_lookup.adapt(data[\"type\"]) #TODO: ensure this doesn't use efficelty use test dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c9f1f-9a6e-4b15-9d6f-606a8226addc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Other Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "713f79d6-63da-427c-9400-3b72721effaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return tf.strings.lower(input_data)\n",
    "\n",
    "def get_normalization_layer(feature_name):\n",
    "    normalization_layer = tf.keras.layers.Normalization(axis=None)\n",
    "    normalization_layer.adapt(data[feature_name].astype(np.float32)) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "    return normalization_layer\n",
    "\n",
    "def get_vectorize_layer(feature_name):\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_features,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=sequence_length\n",
    "    )\n",
    "    vectorize_layer.adapt(data[feature_name]) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "    return vectorize_layer\n",
    "\n",
    "def get_string_lookup(feature_name):\n",
    "    lookup = tf.keras.layers.StringLookup(\n",
    "        output_mode='one_hot',\n",
    "        max_tokens=sequence_length,\n",
    "        pad_to_max_tokens=True,\n",
    "    )\n",
    "    lookup.adapt(data[feature_name]) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056a205-7602-4def-a15d-3a51ff7f32c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Get Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0de43a0-0400-4385-8581-b05e11452db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(feature_names=feature_names):\n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(len(feature_names)):\n",
    "        \n",
    "        feature_name = feature_names[i]\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        if(feature_name == \"latitude\" or feature_name == \"longitude\"):\n",
    "            #outputs.append(flatten(get_normalization_layer(feature_name)(tf.strings.to_number(inputs[:,i], out_type=tf.dtypes.float32))))\n",
    "            inputs.append(tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name))\n",
    "            outputs.append(flatten(get_normalization_layer(feature_name)(tf.strings.to_number(inputs[i], out_type=tf.dtypes.float32))))\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            embedding_layer = tf.keras.layers.Embedding(max_features, 5)\n",
    "            if(feature_name == \"words\" or feature_name == \"aux\"):\n",
    "                layer = get_vectorize_layer(feature_name)\n",
    "            else:\n",
    "                layer = get_string_lookup(feature_name)\n",
    "            #outputs.append(tf.keras.layers.Flatten()(embedding_layer(layer(flatten(inputs[:,i])))))\n",
    "            \n",
    "            inputs.append(tf.keras.Input(shape=(1), dtype=tf.string, name=feature_name))\n",
    "            outputs.append(flatten(embedding_layer(layer(tf.keras.layers.Flatten()(inputs[i])))))\n",
    "    \n",
    "    outputs = tf.concat(outputs, axis=-1)\n",
    "    \n",
    "\n",
    "    sequential_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.5),\n",
    "        #tf.keras.layers.Dense(5, activation='relu'),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=sequential_model(outputs) )\n",
    "\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    sequential_model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb32fe1-6370-4005-ad47-63ed0dbde6f9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0165526-74b7-4db9-80c0-e11f42352ddb",
   "metadata": {},
   "source": [
    "### Define Dataset Getter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c3e7e9f-c56e-4241-b0b0-4316076cc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data=data, feature_names=feature_names):\n",
    "    X = data[feature_names]\n",
    "    y = (string_lookup(list(data[\"type\"])))\n",
    "    #print(str(X.to_dict())[0:1000])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    train_and_validation, test = tf.keras.utils.split_dataset(dataset, right_size=test_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    train, validation = tf.keras.utils.split_dataset(dataset, right_size=validation_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    \n",
    "    print(train)\n",
    "    \n",
    "    return train, validation, test\n",
    "    #return dataset\n",
    "    \n",
    "    \n",
    "def get_X_data(data=data, feature_names=feature_names):\n",
    "    X = data[feature_names]\n",
    "    return X\n",
    "\n",
    "def get_y_data(data=data):\n",
    "    y = string_lookup(list(data[\"type\"]))\n",
    "    return y\n",
    "\n",
    "def get_split_datasets(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17400b5-ffa0-4889-be6f-6b5e195bf3e5",
   "metadata": {},
   "source": [
    "### Define Train Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45bcb0b8-12d0-4ff7-8f48-a81bfcbc0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(X, y):\n",
    "    X_out = {}\n",
    "    y_out = tf.reshape(y, (1,5,))\n",
    "    for i in range(len(feature_names)):\n",
    "        feature_name = feature_names[i]\n",
    "        #print(feature_name, tf.reshape(X[i], (1,)))\n",
    "        X_out[feature_name] = tf.reshape(X[i], (1,))\n",
    "    return X_out, y_out\n",
    "\n",
    "def train_model(model, train, validation):\n",
    "    #kfold = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    histories = []\n",
    "    \n",
    "    \n",
    "    y_classes = np.argmax(np.concatenate([y for _, y in train.map(gen)]), axis=1)\n",
    "    y_labels = np.unique(y_classes)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=y_labels, y=y_classes)\n",
    "    class_weights = dict(zip(y_labels, class_weights))\n",
    "    class_weights[0] = 0\n",
    "    \n",
    "    print(train.map(gen))\n",
    "    history = model.fit(\n",
    "        train.map(gen),\n",
    "        validation_data=validation.map(gen),\n",
    "        epochs=epochs,\n",
    "        #batch_size=batch_size,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)], #TODO: Try commenting this out\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    histories.append(history)\n",
    "        \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6251ebe-b280-47b9-a6eb-e119d8cdad3e",
   "metadata": {},
   "source": [
    "### Generate Feature Combination subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "953808ee-0503-4d89-911d-bfd962447dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_combination_subsets():\n",
    "    feature_names_subsets = []\n",
    "    for N in range(len(feature_names) + 1):\n",
    "         for feature_names_subset in itertools.combinations(feature_names, N): # adapted from: https://stackoverflow.com/questions/464864/get-all-possible-2n-combinations-of-a-list-s-elements-of-any-length\n",
    "            feature_names_subset = list(feature_names_subset)\n",
    "            if(len(feature_names_subset) > 0):\n",
    "                feature_names_subsets.append(feature_names_subset)\n",
    "    return feature_names_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef16df9-f242-4927-80d9-585ae5e9b9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test all feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b694bc23-7200-4884-856e-ee5c0ab4d8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf022c49df34432bb9c98c55778824e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(8,), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float32, name=None))>\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " words (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " aux (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " city (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " region (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " country (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " redirects (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1)            0           ['words[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 1)            0           ['aux[0][0]']                    \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 1)            0           ['city[0][0]']                   \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 1)            0           ['region[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 1)            0           ['country[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 1)            0           ['redirects[0][0]']              \n",
      "                                                                                                  \n",
      " latitude (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " longitude (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 1000)        0           ['flatten_1[0][0]']              \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, 1000)        0           ['flatten_3[0][0]']              \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " string_lookup_3 (StringLookup)  (None, 1000)        0           ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " string_lookup_4 (StringLookup)  (None, 1000)        0           ['flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " string_lookup_5 (StringLookup)  (None, 1000)        0           ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " string_lookup_6 (StringLookup)  (None, 1000)        0           ['flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " tf.strings.to_number (TFOpLamb  (None, 1)           0           ['latitude[0][0]']               \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.strings.to_number_1 (TFOpLa  (None, 1)           0           ['longitude[0][0]']              \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1000, 5)      50000       ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1000, 5)      50000       ['text_vectorization_1[0][0]']   \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1000, 5)      50000       ['string_lookup_3[0][0]']        \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1000, 5)      50000       ['string_lookup_4[0][0]']        \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1000, 5)      50000       ['string_lookup_5[0][0]']        \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 1000, 5)      50000       ['string_lookup_6[0][0]']        \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 1)            3           ['tf.strings.to_number[0][0]']   \n",
      "                                                                                                  \n",
      " normalization_1 (Normalization  (None, 1)           3           ['tf.strings.to_number_1[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 5000)         0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 5000)         0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 5000)         0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 5000)         0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 5000)         0           ['embedding_4[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 5000)         0           ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)           (None, 1)            0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 1)            0           ['normalization_1[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 30002)        0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'flatten_4[0][0]',              \n",
      "                                                                  'flatten_6[0][0]',              \n",
      "                                                                  'flatten_8[0][0]',              \n",
      "                                                                  'flatten_10[0][0]',             \n",
      "                                                                  'flatten_12[0][0]',             \n",
      "                                                                  'flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5)            300085      ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 600,091\n",
      "Trainable params: 600,085\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_14 (Flatten)        (None, 30002)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                300030    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300,085\n",
      "Trainable params: 300,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<_MapDataset element_spec=({'words': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'aux': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'city': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'region': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'country': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'redirects': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'latitude': TensorSpec(shape=(1,), dtype=tf.string, name=None), 'longitude': TensorSpec(shape=(1,), dtype=tf.string, name=None)}, TensorSpec(shape=(1, 5), dtype=tf.float32, name=None))>\n",
      "Epoch 1/25\n",
      "631/631 [==============================] - 8s 9ms/step - loss: 0.4373 - accuracy: 0.8060 - val_loss: 0.3909 - val_accuracy: 0.8089\n",
      "Epoch 2/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.3439 - accuracy: 0.8127 - val_loss: 0.2514 - val_accuracy: 0.8722\n",
      "Epoch 3/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.2462 - accuracy: 0.9094 - val_loss: 0.2061 - val_accuracy: 0.8848\n",
      "Epoch 4/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.1337 - accuracy: 0.9528 - val_loss: 0.1378 - val_accuracy: 0.9405\n",
      "Epoch 5/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0754 - accuracy: 0.9721 - val_loss: 0.1338 - val_accuracy: 0.9468\n",
      "Epoch 6/25\n",
      "631/631 [==============================] - 5s 9ms/step - loss: 0.0485 - accuracy: 0.9803 - val_loss: 0.1205 - val_accuracy: 0.9519\n",
      "Epoch 7/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0407 - accuracy: 0.9832 - val_loss: 0.1138 - val_accuracy: 0.9519\n",
      "Epoch 8/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0342 - accuracy: 0.9883 - val_loss: 0.1157 - val_accuracy: 0.9532\n",
      "Epoch 9/25\n",
      "631/631 [==============================] - 5s 8ms/step - loss: 0.0271 - accuracy: 0.9905 - val_loss: 0.1183 - val_accuracy: 0.9570\n",
      "Epoch 10/25\n",
      "631/631 [==============================] - 5s 9ms/step - loss: 0.0306 - accuracy: 0.9905 - val_loss: 0.1345 - val_accuracy: 0.9506\n",
      "260/260 [==============================] - 1s 4ms/step - loss: 0.0879 - accuracy: 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"<__array_function__ internals>\", line 180, in result_type\n        \n\n    TypeError: data type '-keys' not understood\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_158395/47582142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhistories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved_model/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mtf__save_fn\u001b[0;34m(checkpoint_key)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mmaybe_saveable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveable_factory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"<__array_function__ internals>\", line 180, in result_type\n        \n\n    TypeError: data type '-keys' not understood\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_models\n",
    "models = []\n",
    "feature_names_subsets = generate_feature_combination_subsets()\n",
    "for i in tqdm(range(len(feature_names_subsets)-1, 0, -1)):\n",
    "    #reset_random_seed()\n",
    "    feature_names_subset = feature_names_subsets[i]\n",
    "    #print(feature_names_subset)\n",
    "    #print(\"New Model: \"+\",\".join(feature_names_subset))\n",
    "    \n",
    "    train, validation, test = get_dataset(data, feature_names_subset)\n",
    "    model = get_model(feature_names_subset)\n",
    "    histories = train_model(model, train, validation)\n",
    "    model.evaluate(test.map(gen))\n",
    "    model.save(\"saved_model/\"+\",\".join(feature_names_subset))\n",
    "    break\n",
    "    \n",
    "    #X = get_X_data(data, feature_names_subset)\n",
    "    # y = get_y_data(data)\n",
    "    # X_train, X_test, y_train, y_test = get_split_datasets(X, y)\n",
    "    # model = get_model(feature_names_subset)\n",
    "    # histories = train_model(model, X_train, y_train)\n",
    "    # model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    # tf.saved_model.save(model, \"saved_model/\"+\",\".join(feature_names_subset))\n",
    "    # models.append({\"features_names_subset\":feature_names_subset, \"model\":model, \"histories\":histories})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801e3c8-b5f8-4ed8-a36b-3c7130a18470",
   "metadata": {},
   "source": [
    "## Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c29e03-cf61-4e03-bec1-ef41dd0577f5",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249da21a-b267-4790-874b-c43efa654af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding_to_type_string(one_hot_encoding):\n",
    "    return string_lookup.get_vocabulary()[np.argmax(tf.nn.softmax(one_hot_encoding))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd6dc4-be10-43db-abd8-7bbbb2578cf6",
   "metadata": {},
   "source": [
    "### Define visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7df47c-8b0d-4b44-bfc3-f8d08f731843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_epochs_vs_accuracy_and_loss(model, histories):\n",
    "    acc = []\n",
    "    val_acc = []\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for history in histories:\n",
    "        acc = acc + history.history['accuracy']\n",
    "        val_acc = val_acc + history.history['val_accuracy']\n",
    "\n",
    "        loss = loss + history.history['loss']\n",
    "        val_loss = val_loss + history.history['val_loss']\n",
    "\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    pyplot.figure(figsize=(8, 8))\n",
    "    pyplot.subplot(1, 2, 1)\n",
    "    pyplot.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    pyplot.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    pyplot.legend(loc='lower right')\n",
    "    pyplot.title('Training and Validation Accuracy')\n",
    "\n",
    "    pyplot.subplot(1, 2, 2)\n",
    "    pyplot.plot(epochs_range, loss, label='Training Loss')\n",
    "    pyplot.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.title('Training and Validation Loss')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf16f3e-a940-453e-b25e-f41bee3558e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_prediction_vs_actual_dataframe_table(model):\n",
    "    urls = [ data.loc[i, \"url\"] for i in range(len(X_test)) ]\n",
    "    predictions = [ one_hot_encoding_to_type_string(model.predict(X_test)[i]) for i in range(len(X_test)) ]\n",
    "    actuals = [ one_hot_encoding_to_type_string(y_test[i]) for i in range(len(y_test)) ]\n",
    "\n",
    "    df = pd.DataFrame(np.array([urls, actuals, predictions]).T, columns=[\"url\", \"actual\", \"predicted\"])\n",
    "    df.reset_index(drop=True)\n",
    "    return df[df[\"actual\"] == df[\"predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b26e8b-8e1f-4340-a3bb-fb4c17586be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    features_names_subset = m[\"features_names_subset\"]\n",
    "    model = m[\"model\"]\n",
    "    histories = m[\"histories\"]\n",
    "    print(features_names_subset)\n",
    "    plot_epochs_vs_accuracy_and_loss(model, histories)\n",
    "    show_prediction_vs_actual_dataframe_table(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c272e3f-567b-4f2d-8fc6-f7ad5c7bbc9b",
   "metadata": {},
   "source": [
    "## Predict More (Test) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a0ffd-49fc-4b77-92ad-9a30c7166d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(url):\n",
    "    one = download_one(url)\n",
    "    if(one is None or one[\"status_code\"] != 200):\n",
    "        return \"Failed to get data for '\" + url + \"'\"\n",
    "    one = pd.DataFrame([one.values()], columns=list(one))\n",
    "    one[\"redirects\"] = one[\"redirects\"].astype(str)\n",
    "    one[\"latitude\"] = one[\"latitude\"].astype(str)\n",
    "    one[\"longitude\"] = one[\"longitude\"].astype(str)\n",
    "    \n",
    "    outputs = []\n",
    "    for m in models:\n",
    "        features_names_subset = m[\"features_names_subset\"]\n",
    "        model = m[\"model\"]\n",
    "        histories = m[\"histories\"]\n",
    "        outputs.append(features_names_subset+\": \"+one_hot_encoding_to_type_string(model.predict(one[feature_names])))\n",
    "    return \"\\n\".join(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddcbe0-5843-466a-9dc3-97cf01c9121c",
   "metadata": {},
   "source": [
    "### Test URLs not in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6e37d-fffd-4358-934d-a794d8d6661e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict(\"https://cnn.com/\"))\n",
    "print(predict(\"https://google.com/\"))\n",
    "print(predict(\"https://disneyplus.com/\"))\n",
    "print(predict(\"https://uvm.edu/\"))\n",
    "print(predict(\"https://en.wikipedia.org/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb8d5-801a-4d0b-b298-a047b04217d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict(\"http://www.fwuits.monster/5576HU239G5U8qn613JT25e29A512cY17UFEGsvtwfDm-mEGsi8dRhomSdo6q_s1v06SJOHzt/caretaker-instrumentally\"))\n",
    "print(predict(\"http://www.luieis.beauty/attendee-sinfulness/d8c5V2z395ig86k13i21f62LK4ee2s17QFEGsvtwfDm-mEGsi11dR0om9Reoje6Vok1B08jUHtp1oL\"))\n",
    "print(predict(\"http://www.eiyeid.monster/e3f6K2Q3v95QF8u617c_OTR21f2Peo4ed9T17tFEGsvtwfDm-mEGsi11zRTom9QKoMe6V1Q0_m7aXXqHto/telephoner-plantation\"))\n",
    "print(predict(\"http://irever.live/r.php?q=NzkyNjM0NDs2MTg3OTsxMDA2OzM0OzI7MjAyMy0wMy0yMSAxNTozMDowNTs5OzE7bDs7\"))\n",
    "print(predict(\"https://click.yescaloriedietplan.com/?t=c&ids=NDM5NjExOTY3__NDIxMg%3D%3D__OTg0ODE1ODc%3D__OTAx__1102&url=aHR0cHMlM0ElMkYlMkZ0cmFja2luZy5oZWFsdGhpZXJsaWZ0LmNvbSUyRnpwdGk=\"))\n",
    "print(predict(\"https://links.besttacticalknife.net/a/1625/click/4198/483611/747064e7195ac13a0f72ab11a22cb3f2ad8fa72c/4ec46075f0b8612b13e660aefd0511d442a49206\"))\n",
    "print(predict(\"http://www.folifort.email/l/lt1CK11667E122SG/5494NX7579O10146YB349N80481934SK3221472804\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5662e41-61c1-4340-bc6d-c608377fcfcb",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Train and test a model on every combination of features to see what combinations are most important. - In Progress\n",
    "2. Create a constant seed to reduce random noise-based accuracy changing between models - Done\n",
    "3. Crate a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efc32b-bff3-4c9a-9957-07b308a31746",
   "metadata": {},
   "source": [
    "## Notes for Writeup\n",
    "### Limitation\n",
    "1. Redirects being treated as catagorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18661d-41c8-4305-af53-5daad009dede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
