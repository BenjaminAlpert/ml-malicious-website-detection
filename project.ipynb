{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b298860-8ae7-4686-807b-b6d7d0c9716b",
   "metadata": {},
   "source": [
    "# ML Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0c50c-af2c-47d6-9c31-7d3dd52d677b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958010d-0ed9-40e2-8a1f-85916ede2f5d",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44182d68-eb23-4b21-ad81-66957e153bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pandas numpy matplotlib tensorflow tqdm bs4 IP2Location chardet scikit-learn ipywidgets widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc00bd-b7b6-4f8d-aef3-fbfcaf1da85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!conda install -c conda-forge -y ipywidgets\n",
    "!conda install -c conda-forge -y tqdm\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bdab18-0c41-4e4b-866f-a540ccf1b8e2",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05022e96-8ef9-4e5f-80a6-3faa1d1d8810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import socket\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import os\n",
    "import IP2Location\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"urllib3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8464ad-9975-4e2e-aafe-bbf0088a11be",
   "metadata": {},
   "source": [
    "## Download More Data / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586d269-d052-4278-83c5-aa50142987a7",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7622d0-b100-4b6d-b612-22911428a497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_CHUNK_SIZE = 100\n",
    "TIMEOUT = (10, 10)\n",
    "feature_names = [\"words\", \"aux\", \"city\", \"region\", \"country\", \"redirects\", \"latitude\", \"longitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4592025-f972-40b3-b3a6-bbab3d0c212c",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154d2a2-1c48-4ec9-8e32-0e73bb1ebfde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ip2location_database = IP2Location.IP2Location()\n",
    "ip2location_database.open(os.path.join(\"location_data\", \"IP2LOCATION-LITE-DB11.BIN\"))\n",
    "def get_location(ip_addr=None, hostname=None):\n",
    "    if(ip_addr is None and hostname is not None):\n",
    "        try:\n",
    "            ip_addr = socket.gethostbyname(hostname)\n",
    "        except socket.gaierror:\n",
    "            print(\"Skipped Location Download (Hostname Resolution Error for '\"+hostname+\"')\")\n",
    "            return None\n",
    "    location_data = ip2location_database.get_all(ip_addr)\n",
    "    if(location_data.country_short == \"-\"):\n",
    "        if(not ip_addr in location_database):\n",
    "            location_data = requests.get(\"https://ipinfo.io/\"+ip_addr+\"/json\").json()\n",
    "            if(\"error\" in location_data):\n",
    "                raise Exception(\"Failed because error with download (probably api quota exceded)\")\n",
    "            location_database[ip_addr] = location_data\n",
    "            location_database[ip_addr][\"country_short\"] = location_data[\"country\"]\n",
    "            location_database[ip_addr][\"latitude\"], location_database[ip_addr][\"longitude\"] = tuple(location_data[\"loc\"].split(\",\"))\n",
    "    else:\n",
    "        return ast.literal_eval(str(location_data))\n",
    "    return location_database[ip_addr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c6240-a1d3-49ee-8166-7d3add259a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_url_from_relative(original_url, new_url):\n",
    "    if(new_url.startswith(\"javascript\")):\n",
    "        return None\n",
    "    if(new_url.startswith(\"http\")):\n",
    "        return new_url\n",
    "    url_with_scheme = \"http://\"+original_url if not original_url.startswith(\"http\") else original_url\n",
    "    parsed_url = urlparse(url_with_scheme)\n",
    "    url_scheme = parsed_url.scheme\n",
    "    url_host = parsed_url.netloc\n",
    "    \n",
    "    return urljoin(url_scheme+\"://\"+url_host, new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87109bb-9c26-4b78-a57f-28d53e6b4f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_aux_data(content, original_url):\n",
    "    aux = []\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href')\n",
    "        if(href is not None):\n",
    "            url = get_absolute_url_from_relative(original_url, href)\n",
    "            if(url is not None):\n",
    "                hostname = urlparse(url).netloc\n",
    "                json = get_location(hostname=hostname)\n",
    "                if(json is None):\n",
    "                    continue\n",
    "                try:\n",
    "                    columns = [\n",
    "                        \"hostname_\"+hostname,\n",
    "                        \"ip_addr_\"+json[\"ip\"],\n",
    "                        \"latitude_\"+str(int(float(json[\"latitude\"]))),\n",
    "                        \"longitude_\"+str(int(float(json[\"longitude\"]))),\n",
    "                        \"city_\"+json[\"city\"],\n",
    "                        \"region_\"+json[\"region\"],\n",
    "                        \"country_\"+json[\"country_short\"],\n",
    "                    ]\n",
    "                except KeyError:\n",
    "                    if(\"bogon\" in json and json[\"bogon\"]):\n",
    "                        print(\"Skipped Aux Data Download (Bogon IP)\")\n",
    "                    else:\n",
    "                        print(\"Problem with JSON: \", json)\n",
    "                    columns = []\n",
    "                \n",
    "                columns = [ re.compile('[\\W_]+').sub('_', column) for column in columns ]\n",
    "                \n",
    "                aux = aux + columns\n",
    "                \n",
    "    return \" \".join(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc97b4-6cad-49b4-bc67-bdb5916e5faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_one(original_url, timeout=TIMEOUT):\n",
    "    row = {}\n",
    "    row[\"attempted_download\"] = True\n",
    "\n",
    "    url_with_scheme = \"http://\"+original_url if not original_url.startswith(\"http\") else original_url\n",
    "    parsed_url = urlparse(url_with_scheme)\n",
    "    hostname = parsed_url.netloc\n",
    "    \n",
    "    try:\n",
    "        row[\"hostname\"] = hostname\n",
    "        \n",
    "        ip_addr = socket.gethostbyname(hostname)\n",
    "        row[\"ip_addr\"] = ip_addr\n",
    "\n",
    "        r = requests.get(url_with_scheme, verify=False, timeout=timeout)\n",
    "\n",
    "        row[\"status_code\"] = r.status_code\n",
    "        encoding = chardet.detect(r.content)['encoding']\n",
    "        #if(encoding == None):\n",
    "        #    print(\"Skipped content download (Decoding Error)\")\n",
    "        try:\n",
    "            content = r.content.decode(encoding)\n",
    "            row[\"content\"] = content\n",
    "        except:\n",
    "            try:\n",
    "                content = r.content.decode(\"utf\")\n",
    "            except:\n",
    "                content = None\n",
    "                print(\"Skipped Content Download (Decoding Error)\")\n",
    "            \n",
    "        if(content is not None):\n",
    "            row[\"aux\"] = download_aux_data(content, original_url)\n",
    "            row[\"words\"] = \" \".join(re.compile('[\\W_]+').sub(' ', BeautifulSoup(content, 'html.parser').get_text()).split(\" \"))\n",
    "        \n",
    "        redirects = 0\n",
    "        for r_history in r.history:\n",
    "            if(r_history.status_code == 301):\n",
    "                redirects = redirects + 1\n",
    "        row[\"redirects\"] = redirects\n",
    "\n",
    "        json = get_location(ip_addr=ip_addr)\n",
    "        if(json is None):\n",
    "            return row\n",
    "        try:\n",
    "            row[\"latitude\"] = json[\"latitude\"]\n",
    "            row[\"longitude\"] = json[\"longitude\"]\n",
    "            row[\"city\"] = json[\"city\"]\n",
    "            row[\"region\"] = json[\"region\"]\n",
    "            row[\"country\"] = json[\"country_short\"]\n",
    "\n",
    "        except KeyError:\n",
    "            if(\"bogon\" in json and json[\"bogon\"]):\n",
    "                print(\"Skipped Location Download (Bogon IP)\")\n",
    "            else:\n",
    "                print(\"Problem with JSON: \", json)\n",
    "        \n",
    "    except socket.gaierror:\n",
    "        print(\"Skipped (Hostname Resolution Error for '\"+hostname+\"')\")\n",
    "                      \n",
    "    except socket.error:\n",
    "        print(\"Skipped (Content Download Error for '\"+url_with_scheme+\"')\")\n",
    "        \n",
    "    except UnicodeError:\n",
    "        print(\"Skipped (Unicode Error for '\"+url_with_scheme+\"')\")\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc8223-179e-4c0d-800e-ac4d542f1419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_chunk(chunk_size=DOWNLOAD_CHUNK_SIZE, timeout=TIMEOUT):\n",
    "    global raw_data\n",
    "    if(len(raw_data[raw_data[\"attempted_download\"] == True].index) != 0):\n",
    "          start = raw_data[raw_data[\"attempted_download\"] == True].index[-1]+1\n",
    "    end = start + chunk_size\n",
    "    end = end if len(raw_data[\"url\"]) > end else len(raw_data[\"url\"])\n",
    "    \n",
    "    print(\"Downloading %d more rows ([%d:%d])\" % (chunk_size, start, end))\n",
    "    for row_index in tqdm(range(start, end)):\n",
    "        row = download_one(raw_data.loc[row_index, \"url\"], timeout=timeout)\n",
    "        if(len(list(row)) > 0):\n",
    "            raw_data.loc[row_index, list(row)] = row.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565d439-ff36-4cb4-9932-c34d8ceccddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global raw_data, location_database\n",
    "    \n",
    "    raw_data = pd.read_csv(\"./raw_data.csv\")\n",
    "    raw_data[\"attempted_download\"] = False\n",
    "    \n",
    "    try:\n",
    "        with open(\"./location_data.json\", 'r') as file:\n",
    "            location_database = json.load(file)\n",
    "        file.close()\n",
    "    except FileNotFoundError:\n",
    "        location_database = {}\n",
    "\n",
    "    try:\n",
    "        raw_data = pd.read_csv(\"./data.csv\", index_col=0, low_memory=False)\n",
    "        if(DOWNLOAD_CHUNK_SIZE > 0):\n",
    "            download_chunk()\n",
    "            raw_data.to_csv(\"./data.csv\")\n",
    "            with open(\"./location_data.json\", 'w') as file:\n",
    "                json.dump(location_database, file)\n",
    "            file.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        download_chunk()\n",
    "        raw_data.to_csv(\"./data.csv\")\n",
    "        with open(\"./location_data.json\", 'w') as file:\n",
    "            json.dump(location_database, file)\n",
    "        file.close()\n",
    "\n",
    "    data = raw_data.copy()\n",
    "\n",
    "    data = data.drop(\"attempted_download\", axis=1)\n",
    "    data = data[data[\"status_code\"] == 200]\n",
    "    data = data.drop(\"status_code\", axis=1)\n",
    "    data = data[feature_names + [\"type\", \"url\"]].dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    data[\"redirects\"] = data[\"redirects\"].astype(str)\n",
    "    data[\"latitude\"] = data[\"latitude\"].astype(str)\n",
    "    data[\"longitude\"] = data[\"longitude\"].astype(str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72596d68-54bd-40d4-a875-58ef48bc4d5a",
   "metadata": {},
   "source": [
    "### Load/Download Data and display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abf388-7d09-4fb1-8370-f8ee06be607a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    data = load_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af61902-6f5b-4fea-838f-b6fdb5d8bd09",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b102e1ad-6327-4b86-bf5e-7c0dd3372cdb",
   "metadata": {},
   "source": [
    "### Define Hyperparameter Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d20f1-7727-470a-9131-2a9da2cb21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 1000\n",
    "epochs = 25\n",
    "folds = 5\n",
    "batch_size = 1\n",
    "test_size = 0.33\n",
    "validation_size = 0.20 # ratio after test has been taken out\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b472d45-4a3a-4e4e-b4a6-a5ad9a7674bf",
   "metadata": {},
   "source": [
    "### Ensure Reproducibility (important for feature subsets comparing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197261d-e3d6-4ba7-9198-651bff55072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seed():\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "#reset_random_seed()\n",
    "# Adapted from:\n",
    "# https://stackoverflow.com/questions/45230448/how-to-get-reproducible-result-when-running-keras-with-tensorflow-backend\n",
    "# AND\n",
    "# https://stackoverflow.com/questions/61078946/how-to-get-reproducible-results-keras-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea105ef9-3f9b-40f1-9c32-aa9dc62c754b",
   "metadata": {},
   "source": [
    "### Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83280a3b-80d7-45e6-aae9-2e2b3456dc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      #tf.keras.metrics.TruePositives(name='tp'),\n",
    "      #tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      #tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      #tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      #tf.keras.metrics.Precision(name='precision'),\n",
    "      #tf.keras.metrics.Recall(name='recall'),\n",
    "      #tf.keras.metrics.AUC(name='auc'),\n",
    "      #tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b860ee41-387c-480f-a896-e90e934d8aa6",
   "metadata": {},
   "source": [
    "### Define string_lookup for One Hot Encoding the labels/type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268c603-f12a-46ae-ab8f-b2b75046af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_string_lookup(dataset=None):\n",
    "    def geny(X, y):\n",
    "        return tf.reshape(y, (1,5,))\n",
    "    \n",
    "    string_lookup = tf.keras.layers.StringLookup(output_mode='one_hot')\n",
    "    if(dataset == None):\n",
    "        string_lookup.adapt(data[\"type\"])\n",
    "    else:\n",
    "        string_lookup.adapt(dataset.map(geny))\n",
    "    return string_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b8f47-bfb6-409d-846a-34f19951c18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Gen Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95c8bf-b532-45c3-9bcb-548a13cb102d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen(X, y):\n",
    "    X_out = {}\n",
    "    y_out = tf.reshape(y, (1,5,))\n",
    "    for i in range(len(feature_names)):\n",
    "        feature_name = feature_names[i]\n",
    "        X_out[feature_name] = tf.reshape(X[i], (1,))\n",
    "    return X_out, y_out\n",
    "\n",
    "def genx(X, y=None):\n",
    "        X_out = {}\n",
    "        for i in range(len(feature_names)):\n",
    "            feature_name = feature_names[i]\n",
    "            X_out[feature_name] = tf.reshape(X[i], (1,))\n",
    "        return X_out\n",
    "    \n",
    "def geny(X, y):\n",
    "    return tf.reshape(y, (1,5,))\n",
    "\n",
    "def genx_custom_feature_name(X, y, feature_name, dtype=str):\n",
    "    for i in range(len(feature_names)):\n",
    "        if(feature_name == feature_names[i]):\n",
    "            if(dtype != str):\n",
    "                return tf.strings.to_number(tf.reshape(X[i], (1,)), out_type=tf.dtypes.float32)\n",
    "            else:\n",
    "                return tf.reshape(X[i], (1,))\n",
    "\n",
    "    raise Exception(\"Should never get here\")\n",
    "    return None # should never get here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0056a205-7602-4def-a15d-3a51ff7f32c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Get Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de43a0-0400-4385-8581-b05e11452db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(feature_names=feature_names, train=None):\n",
    "    \n",
    "    def custom_standardization(input_data):\n",
    "        return tf.strings.lower(input_data)\n",
    "\n",
    "    def get_normalization_layer(feature_name):\n",
    "        normalization_layer = tf.keras.layers.Normalization(axis=None)\n",
    "        #print(np.array(train.map(genx_custom_feature_name), dtype=np.float32))\n",
    "        normalization_layer.adapt(train.map(lambda X, y: genx_custom_feature_name(X, y, feature_name, np.float32))) #.astype(np.float32)) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "        return normalization_layer\n",
    "\n",
    "    def get_vectorize_layer(feature_name):\n",
    "        vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "            standardize=custom_standardization,\n",
    "            max_tokens=max_features,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=sequence_length\n",
    "        )\n",
    "        vectorize_layer.adapt(train.map(lambda X, y: genx_custom_feature_name(X, y, feature_name))) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "        return vectorize_layer\n",
    "\n",
    "    def get_string_lookup(feature_name):\n",
    "        lookup = tf.keras.layers.StringLookup(\n",
    "            output_mode='one_hot',\n",
    "            max_tokens=sequence_length,\n",
    "            pad_to_max_tokens=True,\n",
    "        )\n",
    "        lookup.adapt(train.map(lambda X, y: genx_custom_feature_name(X, y, feature_name))) #TODO: ensure this doesn't use efficelty use test dataset for training\n",
    "        return lookup\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(len(feature_names)):\n",
    "        \n",
    "        feature_name = feature_names[i]\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "        if(feature_name == \"latitude\" or feature_name == \"longitude\"):\n",
    "            #outputs.append(flatten(get_normalization_layer(feature_name)(tf.strings.to_number(inputs[:,i], out_type=tf.dtypes.float32))))\n",
    "            layer = get_normalization_layer(feature_name)\n",
    "            inputs.append(tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name))\n",
    "            outputs.append(flatten(layer(tf.strings.to_number(inputs[i], out_type=tf.dtypes.float32))))\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            embedding_layer = tf.keras.layers.Embedding(max_features, 5)\n",
    "            if(feature_name == \"words\" or feature_name == \"aux\"):\n",
    "                layer = get_vectorize_layer(feature_name)\n",
    "            else:\n",
    "                layer = get_string_lookup(feature_name)\n",
    "            #outputs.append(tf.keras.layers.Flatten()(embedding_layer(layer(flatten(inputs[:,i])))))\n",
    "            \n",
    "            inputs.append(tf.keras.Input(shape=(1,), dtype=tf.string, name=feature_name))\n",
    "            outputs.append(flatten(embedding_layer(layer(tf.keras.layers.Flatten()(inputs[i])))))\n",
    "    \n",
    "    outputs = tf.concat(outputs, axis=-1)\n",
    "    \n",
    "\n",
    "    sequential_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='relu'),\n",
    "        #tf.keras.layers.Dropout(0.5),\n",
    "        #tf.keras.layers.Dense(5, activation='relu'),\n",
    "        tf.keras.layers.Dense(5)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=sequential_model(outputs) )\n",
    "\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=METRICS\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    sequential_model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb32fe1-6370-4005-ad47-63ed0dbde6f9",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0165526-74b7-4db9-80c0-e11f42352ddb",
   "metadata": {},
   "source": [
    "### Define Dataset Getter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e7e9f-c56e-4241-b0b0-4316076cc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data=data, feature_names=feature_names):\n",
    "    X = data[feature_names]\n",
    "    y = get_y_string_lookup()(list(data[\"type\"]))\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    \n",
    "    train_and_validation, test = tf.keras.utils.split_dataset(dataset, right_size=test_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    train, validation = tf.keras.utils.split_dataset(dataset, right_size=validation_size, shuffle=True, seed=seed)\n",
    "    \n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17400b5-ffa0-4889-be6f-6b5e195bf3e5",
   "metadata": {},
   "source": [
    "### Define Train Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bcb0b8-12d0-4ff7-8f48-a81bfcbc0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train, validation):\n",
    "    #kfold = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    histories = []\n",
    "    \n",
    "    \n",
    "    y_classes = np.argmax(np.concatenate([y for _, y in train.map(gen)]), axis=1)\n",
    "    y_labels = np.unique(y_classes)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=y_labels, y=y_classes)\n",
    "    class_weights = dict(zip(y_labels, class_weights))\n",
    "    class_weights[0] = 0\n",
    "    \n",
    "    history = model.fit(\n",
    "        train.map(gen),\n",
    "        validation_data=validation.map(gen),\n",
    "        epochs=epochs,\n",
    "        #batch_size=batch_size,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)], #TODO: Try commenting this out\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    histories.append(history)\n",
    "        \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6251ebe-b280-47b9-a6eb-e119d8cdad3e",
   "metadata": {},
   "source": [
    "### Generate Feature Combination subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953808ee-0503-4d89-911d-bfd962447dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_combination_subsets():\n",
    "    feature_names_subsets = []\n",
    "    for N in range(len(feature_names) + 1):\n",
    "         for feature_names_subset in itertools.combinations(feature_names, N): # adapted from: https://stackoverflow.com/questions/464864/get-all-possible-2n-combinations-of-a-list-s-elements-of-any-length\n",
    "            feature_names_subset = list(feature_names_subset)\n",
    "            if(len(feature_names_subset) > 0):\n",
    "                feature_names_subsets.append(feature_names_subset)\n",
    "    return feature_names_subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef16df9-f242-4927-80d9-585ae5e9b9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train and test all feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694bc23-7200-4884-856e-ee5c0ab4d8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p saved_models\n",
    "models = []\n",
    "#feature_names_subsets = generate_feature_combination_subsets()\n",
    "#for i in tqdm(range(len(feature_names_subsets)-1, 0, -1)):\n",
    "#reset_random_seed()\n",
    "#feature_names_subset = feature_names_subsets[i]\n",
    "feature_names_subset = feature_names\n",
    "\n",
    "train, validation, test = get_dataset(data, feature_names_subset)\n",
    "\n",
    "model = get_model(feature_names_subset, train)\n",
    "histories = train_model(model, train, validation)\n",
    "model.evaluate(test.map(gen))\n",
    "models.append({\"features_names_subset\":feature_names_subset, \"model\":model, \"histories\":histories, \"train\": train, \"validation\":validation, \"test\":test})\n",
    "#model.save(\"saved_model/\"+\",\".join(feature_names_subset))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801e3c8-b5f8-4ed8-a36b-3c7130a18470",
   "metadata": {},
   "source": [
    "## Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c29e03-cf61-4e03-bec1-ef41dd0577f5",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249da21a-b267-4790-874b-c43efa654af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding_to_type_string(one_hot_encoding, test):\n",
    "    return get_y_string_lookup().get_vocabulary()[np.argmax(tf.nn.softmax(one_hot_encoding))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd6dc4-be10-43db-abd8-7bbbb2578cf6",
   "metadata": {},
   "source": [
    "### Define visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7df47c-8b0d-4b44-bfc3-f8d08f731843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_epochs_vs_accuracy_and_loss(model, histories):\n",
    "    acc = []\n",
    "    val_acc = []\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for history in histories:\n",
    "        acc = acc + history.history['accuracy']\n",
    "        val_acc = val_acc + history.history['val_accuracy']\n",
    "\n",
    "        loss = loss + history.history['loss']\n",
    "        val_loss = val_loss + history.history['val_loss']\n",
    "\n",
    "\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    pyplot.figure(figsize=(8, 8))\n",
    "    pyplot.subplot(1, 2, 1)\n",
    "    pyplot.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    pyplot.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    pyplot.legend(loc='lower right')\n",
    "    pyplot.title('Training and Validation Accuracy')\n",
    "\n",
    "    pyplot.subplot(1, 2, 2)\n",
    "    pyplot.plot(epochs_range, loss, label='Training Loss')\n",
    "    pyplot.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.title('Training and Validation Loss')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf16f3e-a940-453e-b25e-f41bee3558e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_prediction_vs_actual_dataframe_table(model, test):\n",
    "    \n",
    "    urls = [ data.loc[i, \"url\"] for i in range(len(test.map(genx))) ]\n",
    "    predictions = [ one_hot_encoding_to_type_string(i, test) for i in model.predict(test.map(genx)) ]\n",
    "    actuals = [ one_hot_encoding_to_type_string(i, test) for i in test.map(geny) ]\n",
    "\n",
    "    df = pd.DataFrame(np.array([urls, actuals, predictions]).T, columns=[\"url\", \"actual\", \"predicted\"])\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    with pd.option_context('display.max_rows', 10, 'display.max_columns', 10):\n",
    "        display(df[df[\"actual\"] == df[\"predicted\"]])\n",
    "        display(df[df[\"actual\"] != df[\"predicted\"]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b26e8b-8e1f-4340-a3bb-fb4c17586be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    features_names_subset = m[\"features_names_subset\"]\n",
    "    model = m[\"model\"]\n",
    "    histories = m[\"histories\"]\n",
    "    test = m[\"test\"]\n",
    "    plot_epochs_vs_accuracy_and_loss(model, histories)\n",
    "    show_prediction_vs_actual_dataframe_table(model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c272e3f-567b-4f2d-8fc6-f7ad5c7bbc9b",
   "metadata": {},
   "source": [
    "## Predict More (Test) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a0ffd-49fc-4b77-92ad-9a30c7166d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(url):\n",
    "    one = download_one(url)\n",
    "    if(one is None or (\"status_code\" in one and one[\"status_code\"] != 200) or not any([feature_name in one for feature_name in feature_names])):\n",
    "        return \"Failed to get data for '\" + url + \"'\"\n",
    "    one = pd.DataFrame([one.values()], columns=list(one))\n",
    "    one[\"redirects\"] = one[\"redirects\"].astype(str)\n",
    "    one[\"latitude\"] = one[\"latitude\"].astype(str)\n",
    "    one[\"longitude\"] = one[\"longitude\"].astype(str)\n",
    "    \n",
    "    tfds = tf.data.Dataset.from_tensor_slices(one[feature_names])\n",
    "    \n",
    "    outputs = []\n",
    "    for m in models:\n",
    "        features_names_subset = m[\"features_names_subset\"]\n",
    "        model = m[\"model\"]\n",
    "        histories = m[\"histories\"]\n",
    "        #outputs.append(\", \".join(features_names_subset)+\": \"+one_hot_encoding_to_type_string(model.predict(tfds.map(genx)), test))\n",
    "        outputs.append(one_hot_encoding_to_type_string(model.predict(tfds.map(genx), verbose=None), test))\n",
    "    return \"\\n\".join(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddcbe0-5843-466a-9dc3-97cf01c9121c",
   "metadata": {},
   "source": [
    "### Test URLs not in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6e37d-fffd-4358-934d-a794d8d6661e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict(\"https://cnn.com/\"))\n",
    "print(predict(\"https://google.com/\"))\n",
    "print(predict(\"https://disneyplus.com/\"))\n",
    "print(predict(\"https://uvm.edu/\"))\n",
    "print(predict(\"https://en.wikipedia.org/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fb8d5-801a-4d0b-b298-a047b04217d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict(\"http://irever.live/r.php?q=NzkyNjM0NDs2MTg3OTsxMDA2OzM0OzI7MjAyMy0wMy0yMSAxNTozMDowNTs5OzE7bDs7\"))\n",
    "print(predict(\"https://click.yescaloriedietplan.com/?t=c&ids=NDM5NjExOTY3__NDIxMg%3D%3D__OTg0ODE1ODc%3D__OTAx__1102&url=aHR0cHMlM0ElMkYlMkZ0cmFja2luZy5oZWFsdGhpZXJsaWZ0LmNvbSUyRnpwdGk=\"))\n",
    "print(predict(\"http://www.folifort.email/l/lt1CK11667E122SG/5494NX7579O10146YB349N80481934SK3221472804\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5662e41-61c1-4340-bc6d-c608377fcfcb",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Train and test a model on every combination of features to see what combinations are most important. - In Progress\n",
    "2. Create a constant seed to reduce random noise-based accuracy changing between models - Done\n",
    "3. Crate a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efc32b-bff3-4c9a-9957-07b308a31746",
   "metadata": {},
   "source": [
    "## Notes for Writeup\n",
    "### Limitation\n",
    "1. Redirects being treated as catagorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18661d-41c8-4305-af53-5daad009dede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
